{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-02-20T10:47:13.144663Z",
     "iopub.status.busy": "2022-02-20T10:47:13.144373Z",
     "iopub.status.idle": "2022-02-20T10:47:14.375336Z",
     "shell.execute_reply": "2022-02-20T10:47:14.374638Z",
     "shell.execute_reply.started": "2022-02-20T10:47:13.144586Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy.misc import imread, imresize\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T10:47:14.377076Z",
     "iopub.status.busy": "2022-02-20T10:47:14.376859Z",
     "iopub.status.idle": "2022-02-20T10:47:18.791194Z",
     "shell.execute_reply": "2022-02-20T10:47:18.790016Z",
     "shell.execute_reply.started": "2022-02-20T10:47:14.377043Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "# tf.set_random_seed(30)\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T10:47:31.817622Z",
     "iopub.status.busy": "2022-02-20T10:47:31.817356Z",
     "iopub.status.idle": "2022-02-20T10:47:31.821332Z",
     "shell.execute_reply": "2022-02-20T10:47:31.820561Z",
     "shell.execute_reply.started": "2022-02-20T10:47:31.817592Z"
    }
   },
   "outputs": [],
   "source": [
    "base_dir= 'kaggle/input/gesture-recognition/Project_data/'\n",
    "model_save_dir= 'kaggle/working/models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T10:47:32.969116Z",
     "iopub.status.busy": "2022-02-20T10:47:32.968428Z",
     "iopub.status.idle": "2022-02-20T10:47:33.000546Z",
     "shell.execute_reply": "2022-02-20T10:47:32.999908Z",
     "shell.execute_reply.started": "2022-02-20T10:47:32.969081Z"
    }
   },
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open(base_dir+'train.csv').readlines())\n",
    "val_doc = np.random.permutation(open(base_dir+'train.csv').readlines())\n",
    "batch_size = 30 #experiment with the batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T10:47:34.646174Z",
     "iopub.status.busy": "2022-02-20T10:47:34.645501Z",
     "iopub.status.idle": "2022-02-20T10:47:34.662764Z",
     "shell.execute_reply": "2022-02-20T10:47:34.662049Z",
     "shell.execute_reply.started": "2022-02-20T10:47:34.646135Z"
    }
   },
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', '/'+source_path, '; batch size =', batch_size)\n",
    "    img_idx = np.round(np.linspace(0, 29, num_frames_to_sample)).astype(int)\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(t)//batch_size\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            \n",
    "            batch_data = np.zeros((batch_size, len(img_idx), img_height, img_width, 3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            \n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                \n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    image_resized = imresize(image,(img_height, img_width)).astype(np.float32)\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (image_resized[:,:,0])/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = (image_resized[:,:,1])/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = (image_resized[:,:,2])/255 #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        remaining_seq = len(t)%batch_size\n",
    "        if remaining_seq != 0:\n",
    "            for folder in range(remaining_seq):\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + ((num_batches)*batch_size)].split(';')[0])\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + ((num_batches)*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    image_resized = imresize(image,(img_height, img_width)).astype(np.float32)\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = image_resized[:,:,0]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = image_resized[:,:,1]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = image_resized[:,:,2]/255 #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (num_batches*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T10:47:36.920877Z",
     "iopub.status.busy": "2022-02-20T10:47:36.920177Z",
     "iopub.status.idle": "2022-02-20T10:47:36.926511Z",
     "shell.execute_reply": "2022-02-20T10:47:36.925607Z",
     "shell.execute_reply.started": "2022-02-20T10:47:36.920842Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 663\n",
      "# epochs = 10\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "\n",
    "train_path = base_dir+'train'\n",
    "val_path = base_dir+'val'\n",
    "\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "\n",
    "num_epochs = 10 # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T10:47:38.344784Z",
     "iopub.status.busy": "2022-02-20T10:47:38.344247Z",
     "iopub.status.idle": "2022-02-20T10:47:38.348931Z",
     "shell.execute_reply": "2022-02-20T10:47:38.347913Z",
     "shell.execute_reply.started": "2022-02-20T10:47:38.344737Z"
    }
   },
   "outputs": [],
   "source": [
    "num_classes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T10:47:39.201675Z",
     "iopub.status.busy": "2022-02-20T10:47:39.201406Z",
     "iopub.status.idle": "2022-02-20T10:47:39.207464Z",
     "shell.execute_reply": "2022-02-20T10:47:39.206383Z",
     "shell.execute_reply.started": "2022-02-20T10:47:39.201643Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, SimpleRNN, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D, Conv2D, MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.layers.recurrent import LSTM , GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T10:47:41.442256Z",
     "iopub.status.busy": "2022-02-20T10:47:41.441997Z",
     "iopub.status.idle": "2022-02-20T10:47:41.451249Z",
     "shell.execute_reply": "2022-02-20T10:47:41.450604Z",
     "shell.execute_reply.started": "2022-02-20T10:47:41.442227Z"
    }
   },
   "outputs": [],
   "source": [
    "# Moving the code to train any model into a function for simplictiy\n",
    "def train_model(model, batch_size, num_epochs):\n",
    "    train_generator = generator(train_path, train_doc, batch_size)\n",
    "    val_generator = generator(val_path, val_doc, batch_size)\n",
    "    \n",
    "    curr_dt_time = datetime.datetime.now()\n",
    "    model_name = model_save_dir+'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "    if not os.path.exists(model_name):\n",
    "        os.makedirs(model_name)\n",
    "\n",
    "    filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "    LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4) # write the REducelronplateau code here\n",
    "    callbacks_list = [checkpoint, LR]\n",
    "\n",
    "        \n",
    "    if (num_train_sequences%batch_size) == 0:\n",
    "        steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "    else:\n",
    "        steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "    if (num_val_sequences%batch_size) == 0:\n",
    "        validation_steps = int(num_val_sequences/batch_size)\n",
    "    else:\n",
    "        validation_steps = (num_val_sequences//batch_size) + 1\n",
    "        \n",
    "    history = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n",
    "    \n",
    "    K.clear_session()\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T10:47:42.523944Z",
     "iopub.status.busy": "2022-02-20T10:47:42.523056Z",
     "iopub.status.idle": "2022-02-20T10:47:42.528384Z",
     "shell.execute_reply": "2022-02-20T10:47:42.527673Z",
     "shell.execute_reply.started": "2022-02-20T10:47:42.523894Z"
    }
   },
   "outputs": [],
   "source": [
    "num_frames_to_sample = 30\n",
    "img_height, img_width = 160, 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T01:02:16.070383Z",
     "iopub.status.busy": "2022-02-20T01:02:16.069739Z",
     "iopub.status.idle": "2022-02-20T01:02:17.163405Z",
     "shell.execute_reply": "2022-02-20T01:02:17.161968Z",
     "shell.execute_reply.started": "2022-02-20T01:02:16.070340Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-20 01:02:16.118988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-20 01:02:16.147987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-20 01:02:16.148734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-20 01:02:16.150474: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-20 01:02:16.150818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-20 01:02:16.151517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-20 01:02:16.152206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-20 01:02:16.783029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-20 01:02:16.783845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-20 01:02:16.784522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-20 01:02:16.785114: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15403 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    }
   ],
   "source": [
    "#write your model here\n",
    "\n",
    "model_1 = Sequential()\n",
    "\n",
    "model_1.add(Conv3D(16, (3, 3, 3), padding='same', input_shape=(num_frames_to_sample, img_height, img_width, 3)))\n",
    "model_1.add(Activation('relu'))\n",
    "model_1.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_1.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
    "model_1.add(Activation('relu'))\n",
    "model_1.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_1.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
    "model_1.add(Activation('relu'))\n",
    "model_1.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_1.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
    "model_1.add(Activation('relu'))\n",
    "model_1.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_1.add(Flatten())\n",
    "model_1.add(Dense(128,activation='relu'))\n",
    "\n",
    "model_1.add(Dense(64,activation='relu'))\n",
    "\n",
    "model_1.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to compile the model. When you print the summary of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d (Conv3D)              (None, 30, 160, 160, 16)  1312      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 30, 160, 160, 16)  0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d (MaxPooling3D) (None, 15, 80, 80, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 15, 80, 80, 32)    4128      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 15, 80, 80, 32)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 7, 40, 40, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 7, 40, 40, 64)     16448     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 7, 40, 40, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 3, 20, 20, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 3, 20, 20, 128)    65664     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 3, 20, 20, 128)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 1, 10, 10, 128)    0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               1638528   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 1,734,661\n",
      "Trainable params: 1,734,661\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = tf.keras.optimizers.Adam()\n",
    "model_1.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /kaggle/input/gesture-recognition/Project_data/train ; batch size = 10\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.5638 - categorical_accuracy: 0.2493Source path =  /kaggle/input/gesture-recognition/Project_data/val ; batch size = 10\n",
      "67/67 [==============================] - 353s 5s/step - loss: 1.5638 - categorical_accuracy: 0.2493 - val_loss: 1.3192 - val_categorical_accuracy: 0.4400\n",
      "\n",
      "Epoch 00001: saving model to /kaggle/working/models/model_init_2022-02-1900_11_43.534839/model-00001-1.56377-0.24925-1.31916-0.44000.h5\n",
      "Source path =  /kaggle/input/gesture-recognition/Project_data/train ; batch size = 20\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.2554 - categorical_accuracy: 0.4588Source path =  /kaggle/input/gesture-recognition/Project_data/val ; batch size = 20\n",
      "34/34 [==============================] - 253s 8s/step - loss: 1.2554 - categorical_accuracy: 0.4588 - val_loss: 0.9956 - val_categorical_accuracy: 0.5600\n",
      "\n",
      "Epoch 00001: saving model to /kaggle/working/models/model_init_2022-02-1900_11_43.534839/model-00001-1.25543-0.45882-0.99559-0.56000.h5\n",
      "Source path =  /kaggle/input/gesture-recognition/Project_data/train ; batch size = 30\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.0512 - categorical_accuracy: 0.5333Source path =  /kaggle/input/gesture-recognition/Project_data/val ; batch size = 30\n",
      "23/23 [==============================] - 254s 11s/step - loss: 1.0512 - categorical_accuracy: 0.5333 - val_loss: 1.1763 - val_categorical_accuracy: 0.5750\n",
      "\n",
      "Epoch 00001: saving model to /kaggle/working/models/model_init_2022-02-1900_11_43.534839/model-00001-1.05119-0.53333-1.17626-0.57500.h5\n",
      "Source path =  /kaggle/input/gesture-recognition/Project_data/train ; batch size = 40\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[40,16,30,160,160] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node sequential/conv3d/Conv3D (defined at <ipython-input-10-91cd96f32b93>:32) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_1010]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-32077b828f13>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     56\u001b[0m Epoch 00001: saving model to /kaggle/working/models/model_init_2022-02-1900_11_43.534839/model-00001-1.05119-0.53333-1.17626-0.57500.h5''')\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-32077b828f13>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, batch_size, num_epochs)\u001b[0m\n\u001b[0;32m     34\u001b[0m     history = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n\u001b[0;32m     35\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m                     validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Desktop\\Projects\\Freelance\\Neural Network Project\\venv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1987\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1988\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1989\u001b[1;33m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1990\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1991\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mdoc_controls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_not_generate_docs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Desktop\\Projects\\Freelance\\Neural Network Project\\venv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Desktop\\Projects\\Freelance\\Neural Network Project\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Desktop\\Projects\\Freelance\\Neural Network Project\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Desktop\\Projects\\Freelance\\Neural Network Project\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 3040\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3042\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Desktop\\Projects\\Freelance\\Neural Network Project\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1964\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Desktop\\Projects\\Freelance\\Neural Network Project\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    597\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mD:\\Desktop\\Projects\\Freelance\\Neural Network Project\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[40,16,30,160,160] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node sequential/conv3d/Conv3D (defined at <ipython-input-10-91cd96f32b93>:32) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_1010]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "# # Experimenting for maximum batch_size when img_height, img_width = 160, 160\n",
    "for batch_size in range(10, 50, 10):\n",
    "    history = train_model(model_1, batch_size = batch_size, num_epochs = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Using Conv3D\n",
    "\n",
    "### Model 1 (Conv3D)\n",
    "    batch_size = 30\n",
    "    num_epochs = 10\n",
    "    img_height, img_width = 160, 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T01:02:29.693188Z",
     "iopub.status.busy": "2022-02-20T01:02:29.692472Z",
     "iopub.status.idle": "2022-02-20T01:02:29.697200Z",
     "shell.execute_reply": "2022-02-20T01:02:29.696537Z",
     "shell.execute_reply.started": "2022-02-20T01:02:29.693150Z"
    }
   },
   "outputs": [],
   "source": [
    "num_frames_to_sample = 30\n",
    "img_height, img_width = 160, 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T01:02:30.859640Z",
     "iopub.status.busy": "2022-02-20T01:02:30.859092Z",
     "iopub.status.idle": "2022-02-20T01:02:31.019086Z",
     "shell.execute_reply": "2022-02-20T01:02:31.015812Z",
     "shell.execute_reply.started": "2022-02-20T01:02:30.859603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_8 (Conv3D)            (None, 30, 160, 160, 16)  1312      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 30, 160, 160, 16)  0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_8 (MaxPooling3 (None, 15, 80, 80, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_9 (Conv3D)            (None, 15, 80, 80, 32)    4128      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 15, 80, 80, 32)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_9 (MaxPooling3 (None, 7, 40, 40, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_10 (Conv3D)           (None, 7, 40, 40, 64)     16448     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 7, 40, 40, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_10 (MaxPooling (None, 3, 20, 20, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_11 (Conv3D)           (None, 3, 20, 20, 128)    65664     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 3, 20, 20, 128)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_11 (MaxPooling (None, 1, 10, 10, 128)    0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               1638528   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 1,734,661\n",
      "Trainable params: 1,734,661\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1 = Sequential()\n",
    "\n",
    "model_1.add(Conv3D(16, (3, 3, 3), padding='same', input_shape=(num_frames_to_sample, img_height, img_width, 3)))\n",
    "model_1.add(Activation('relu'))\n",
    "model_1.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_1.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
    "model_1.add(Activation('relu'))\n",
    "model_1.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_1.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
    "model_1.add(Activation('relu'))\n",
    "model_1.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_1.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
    "model_1.add(Activation('relu'))\n",
    "model_1.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_1.add(Flatten())\n",
    "model_1.add(Dense(128,activation='relu'))\n",
    "\n",
    "model_1.add(Dense(64,activation='relu'))\n",
    "\n",
    "model_1.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "optimiser = tf.keras.optimizers.Adam()\n",
    "model_1.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T01:02:35.613521Z",
     "iopub.status.busy": "2022-02-20T01:02:35.613271Z",
     "iopub.status.idle": "2022-02-20T01:45:28.870491Z",
     "shell.execute_reply": "2022-02-20T01:45:28.869487Z",
     "shell.execute_reply.started": "2022-02-20T01:02:35.613491Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /kaggle/input/gesture-recognition/Project_data/train ; batch size = 30\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.6176 - categorical_accuracy: 0.2145Source path =  /kaggle/input/gesture-recognition/Project_data/val ; batch size = 30\n",
      "23/23 [==============================] - 261s 12s/step - loss: 1.6176 - categorical_accuracy: 0.2145 - val_loss: 1.6631 - val_categorical_accuracy: 0.2250\n",
      "\n",
      "Epoch 00001: saving model to /kaggle/working/models/model_init_2022-02-1906_56_53.421991/model-00001-1.61757-0.21449-1.66309-0.22500.h5\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 256s 12s/step - loss: 1.4685 - categorical_accuracy: 0.3435 - val_loss: 1.3946 - val_categorical_accuracy: 0.4333\n",
      "\n",
      "Epoch 00002: saving model to /kaggle/working/models/model_init_2022-02-1906_56_53.421991/model-00002-1.46848-0.34348-1.39464-0.43333.h5\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 258s 12s/step - loss: 1.2490 - categorical_accuracy: 0.4681 - val_loss: 1.0850 - val_categorical_accuracy: 0.6417\n",
      "\n",
      "Epoch 00003: saving model to /kaggle/working/models/model_init_2022-02-1906_56_53.421991/model-00003-1.24901-0.46812-1.08497-0.64167.h5\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 250s 11s/step - loss: 1.1172 - categorical_accuracy: 0.5493 - val_loss: 1.3436 - val_categorical_accuracy: 0.5167\n",
      "\n",
      "Epoch 00004: saving model to /kaggle/working/models/model_init_2022-02-1906_56_53.421991/model-00004-1.11721-0.54928-1.34360-0.51667.h5\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 256s 12s/step - loss: 0.9789 - categorical_accuracy: 0.6000 - val_loss: 1.1283 - val_categorical_accuracy: 0.5583\n",
      "\n",
      "Epoch 00005: saving model to /kaggle/working/models/model_init_2022-02-1906_56_53.421991/model-00005-0.97895-0.60000-1.12832-0.55833.h5\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 255s 12s/step - loss: 0.7294 - categorical_accuracy: 0.7101 - val_loss: 1.0548 - val_categorical_accuracy: 0.6583\n",
      "\n",
      "Epoch 00006: saving model to /kaggle/working/models/model_init_2022-02-19106_56_53.421991/model-00006-0.72940-0.71014-1.05476-0.65833.h5\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 257s 12s/step - loss: 0.6838 - categorical_accuracy: 0.7435 - val_loss: 1.0112 - val_categorical_accuracy: 0.6667\n",
      "\n",
      "Epoch 00007: saving model to /kaggle/working/models/model_init_2022-02-1906_56_53.421991/model-00007-0.68384-0.74348-1.01120-0.66667.h5\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 252s 11s/step - loss: 0.4841 - categorical_accuracy: 0.8116 - val_loss: 1.8697 - val_categorical_accuracy: 0.5750\n",
      "\n",
      "Epoch 00008: saving model to /kaggle/working/models/model_init_2022-02-1906_56_53.421991/model-00008-0.48406-0.81159-1.86970-0.57500.h5\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 261s 12s/step - loss: 0.4354 - categorical_accuracy: 0.8493 - val_loss: 0.8753 - val_categorical_accuracy: 0.7583\n",
      "\n",
      "Epoch 00009: saving model to /kaggle/working/models/model_init_2022-02-1906_56_53.421991/model-00009-0.43542-0.84928-0.87533-0.75833.h5\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 258s 12s/step - loss: 0.3312 - categorical_accuracy: 0.8928 - val_loss: 1.0111 - val_categorical_accuracy: 0.7417\n",
      "\n",
      "Epoch 00010: saving model to /kaggle/working/models/model_init_2022-02-1906_56_53.421991/model-00010-0.33116-0.89275-1.01112-0.74167.h5\n"
     ]
    }
   ],
   "source": [
    "history_1 = train_model(model_1, batch_size = 30, num_epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 (Conv3D)\n",
    "    batch_size = 30\n",
    "    num_epochs = 10\n",
    "    img_height, img_width = 160, 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T01:45:29.811792Z",
     "iopub.status.busy": "2022-02-20T01:45:29.811446Z",
     "iopub.status.idle": "2022-02-20T01:45:29.824180Z",
     "shell.execute_reply": "2022-02-20T01:45:29.823085Z",
     "shell.execute_reply.started": "2022-02-20T01:45:29.811739Z"
    }
   },
   "outputs": [],
   "source": [
    "num_frames_to_sample = 30\n",
    "img_height, img_width = 160, 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T01:45:29.830256Z",
     "iopub.status.busy": "2022-02-20T01:45:29.829417Z",
     "iopub.status.idle": "2022-02-20T01:45:30.047605Z",
     "shell.execute_reply": "2022-02-20T01:45:30.046792Z",
     "shell.execute_reply.started": "2022-02-20T01:45:29.830217Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_12 (Conv3D)           (None, 30, 160, 160, 16)  1312      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 30, 160, 160, 16)  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 30, 160, 160, 16)  64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_12 (MaxPooling (None, 15, 80, 80, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_13 (Conv3D)           (None, 15, 80, 80, 32)    13856     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 15, 80, 80, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 15, 80, 80, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_13 (MaxPooling (None, 7, 40, 40, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_14 (Conv3D)           (None, 7, 40, 40, 64)     55360     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 7, 40, 40, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 7, 40, 40, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_14 (MaxPooling (None, 3, 20, 20, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_15 (Conv3D)           (None, 3, 20, 20, 128)    221312    \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 3, 20, 20, 128)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 3, 20, 20, 128)    512       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_15 (MaxPooling (None, 1, 10, 10, 128)    0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               1638528   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 1,949,509\n",
      "Trainable params: 1,948,517\n",
      "Non-trainable params: 992\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2 = Sequential()\n",
    "\n",
    "model_2.add(Conv3D(16, (3,3,3), padding='same', input_shape=(num_frames_to_sample, img_height, img_width,3)))\n",
    "model_2.add(Activation('relu'))\n",
    "model_2.add(BatchNormalization())\n",
    "model_2.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_2.add(Conv3D(32, (3,3,3), padding='same'))\n",
    "model_2.add(Activation('relu'))\n",
    "model_2.add(BatchNormalization())\n",
    "model_2.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_2.add(Conv3D(64, (3,3,3), padding='same'))\n",
    "model_2.add(Activation('relu'))\n",
    "model_2.add(BatchNormalization())\n",
    "model_2.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_2.add(Conv3D(128, (3,3,3), padding='same'))\n",
    "model_2.add(Activation('relu'))\n",
    "model_2.add(BatchNormalization())\n",
    "model_2.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_2.add(Flatten())\n",
    "\n",
    "model_2.add(Dense(128,activation='relu'))\n",
    "model_2.add(BatchNormalization())\n",
    "model_2.add(Dropout(0.1))\n",
    "\n",
    "model_2.add(Dense(128,activation='relu'))\n",
    "model_2.add(BatchNormalization())\n",
    "model_2.add(Dropout(0.1))\n",
    "\n",
    "model_2.add(Dense(num_classes,activation='softmax'))\n",
    "\n",
    "optimiser = tf.keras.optimizers.Adam()\n",
    "model_2.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T01:45:30.049234Z",
     "iopub.status.busy": "2022-02-20T01:45:30.048902Z",
     "iopub.status.idle": "2022-02-20T02:29:02.435141Z",
     "shell.execute_reply": "2022-02-20T02:29:02.434376Z",
     "shell.execute_reply.started": "2022-02-20T01:45:30.049194Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /kaggle/input/gesture-recognition/Project_data/train ; batch size = 30\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3513 - categorical_accuracy: 0.5072Source path =  /kaggle/input/gesture-recognition/Project_data/val ; batch size = 30\n",
      "23/23 [==============================] - 260s 12s/step - loss: 1.3513 - categorical_accuracy: 0.5072 - val_loss: 2.0041 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00001: saving model to /kaggle/working/models/model_init_2022-02-1908_15_45.231892/model-00001-1.35128-0.50725-2.00409-0.20000.h5\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 259s 12s/step - loss: 0.7465 - categorical_accuracy: 0.7435 - val_loss: 2.4798 - val_categorical_accuracy: 0.1833\n",
      "\n",
      "Epoch 00002: saving model to /kaggle/working/models/model_init_2022-02-1908_15_45.231892/model-00002-0.74645-0.74348-2.47980-0.18333.h5\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 256s 12s/step - loss: 0.4605 - categorical_accuracy: 0.8478 - val_loss: 2.4439 - val_categorical_accuracy: 0.1667\n",
      "\n",
      "Epoch 00003: saving model to /kaggle/working/models/model_init_2022-02-1908_15_45.231892/model-00003-0.46048-0.84783-2.44390-0.16667.h5\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 251s 11s/step - loss: 0.2681 - categorical_accuracy: 0.9232 - val_loss: 3.2733 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00004: saving model to /kaggle/working/models/model_init_2022-02-1908_15_45.231892/model-00004-0.26809-0.92319-3.27325-0.20000.h5\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 258s 12s/step - loss: 0.2043 - categorical_accuracy: 0.9536 - val_loss: 3.5948 - val_categorical_accuracy: 0.1917\n",
      "\n",
      "Epoch 00005: saving model to /kaggle/working/models/model_init_2022-02-1908_15_45.231892/model-00005-0.20430-0.95362-3.59476-0.19167.h5\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 261s 12s/step - loss: 0.1203 - categorical_accuracy: 0.9754 - val_loss: 3.5286 - val_categorical_accuracy: 0.1750\n",
      "\n",
      "Epoch 00006: saving model to /kaggle/working/models/model_init_2022-02-1908_15_45.231892/model-00006-0.12025-0.97536-3.52863-0.17500.h5\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 260s 12s/step - loss: 0.0952 - categorical_accuracy: 0.9928 - val_loss: 3.4523 - val_categorical_accuracy: 0.2750\n",
      "\n",
      "Epoch 00007: saving model to /kaggle/working/models/model_init_2022-02-1908_15_45.231892/model-00007-0.09516-0.99275-3.45232-0.27500.h5\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 252s 11s/step - loss: 0.0750 - categorical_accuracy: 0.9971 - val_loss: 4.4797 - val_categorical_accuracy: 0.2083\n",
      "\n",
      "Epoch 00008: saving model to /kaggle/working/models/model_init_2022-02-1908_15_45.231892/model-00008-0.07500-0.99710-4.47968-0.20833.h5\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 260s 12s/step - loss: 0.0754 - categorical_accuracy: 0.9957 - val_loss: 5.0131 - val_categorical_accuracy: 0.2250\n",
      "\n",
      "Epoch 00009: saving model to /kaggle/working/models/model_init_2022-02-1908_15_45.231892/model-00009-0.07544-0.99565-5.01310-0.22500.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 260s 12s/step - loss: 0.0550 - categorical_accuracy: 0.9971 - val_loss: 5.2408 - val_categorical_accuracy: 0.2417\n",
      "\n",
      "Epoch 00010: saving model to /kaggle/working/models/model_init_2022-02-1908_15_45.231892/model-00010-0.05501-0.99710-5.24080-0.24167.h5\n"
     ]
    }
   ],
   "source": [
    "history_2 = train_model(model_2, batch_size = 30, num_epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 (Conv3D)\n",
    "    batch_size = 20\n",
    "    num_epochs = 20\n",
    "    img_height, img_width = 160, 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T02:29:02.768725Z",
     "iopub.status.busy": "2022-02-20T02:29:02.768447Z",
     "iopub.status.idle": "2022-02-20T02:29:02.773028Z",
     "shell.execute_reply": "2022-02-20T02:29:02.771955Z",
     "shell.execute_reply.started": "2022-02-20T02:29:02.768688Z"
    }
   },
   "outputs": [],
   "source": [
    "num_frames_to_sample = 30\n",
    "img_height, img_width = 160, 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T02:29:02.774803Z",
     "iopub.status.busy": "2022-02-20T02:29:02.774512Z",
     "iopub.status.idle": "2022-02-20T02:29:02.943158Z",
     "shell.execute_reply": "2022-02-20T02:29:02.942452Z",
     "shell.execute_reply.started": "2022-02-20T02:29:02.774765Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_16 (Conv3D)           (None, 30, 160, 160, 16)  400       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 30, 160, 160, 16)  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 30, 160, 160, 16)  64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_16 (MaxPooling (None, 15, 80, 80, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_17 (Conv3D)           (None, 15, 80, 80, 32)    4128      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 15, 80, 80, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 15, 80, 80, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_17 (MaxPooling (None, 7, 40, 40, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_18 (Conv3D)           (None, 7, 40, 40, 64)     16448     \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 7, 40, 40, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 7, 40, 40, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_18 (MaxPooling (None, 3, 20, 20, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_19 (Conv3D)           (None, 3, 20, 20, 128)    65664     \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 3, 20, 20, 128)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 3, 20, 20, 128)    512       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_19 (MaxPooling (None, 1, 10, 10, 128)    0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 256)               3277056   \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 3,433,781\n",
      "Trainable params: 3,432,277\n",
      "Non-trainable params: 1,504\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_3 = Sequential()\n",
    "model_3.add(Conv3D(16, (2,2,2), padding='same', input_shape=(num_frames_to_sample, img_height, img_width,3)))\n",
    "model_3.add(Activation('relu'))\n",
    "model_3.add(BatchNormalization())\n",
    "model_3.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_3.add(Conv3D(32, (2,2,2), padding='same'))\n",
    "model_3.add(Activation('relu'))\n",
    "model_3.add(BatchNormalization())\n",
    "model_3.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_3.add(Conv3D(64, (2,2,2), padding='same'))\n",
    "model_3.add(Activation('relu'))\n",
    "model_3.add(BatchNormalization())\n",
    "model_3.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_3.add(Conv3D(128, (2,2,2), padding='same'))\n",
    "model_3.add(Activation('relu'))\n",
    "model_3.add(BatchNormalization())\n",
    "model_3.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_3.add(Flatten())\n",
    "\n",
    "model_3.add(Dense(256,activation='relu'))\n",
    "model_3.add(BatchNormalization())\n",
    "model_3.add(Dropout(0.5))\n",
    "\n",
    "model_3.add(Dense(256,activation='relu'))\n",
    "model_3.add(BatchNormalization())\n",
    "model_3.add(Dropout(0.5))\n",
    "\n",
    "model_3.add(Dense(num_classes,activation='softmax'))\n",
    "\n",
    "optimiser = tf.keras.optimizers.Adam()\n",
    "model_3.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T02:29:02.944691Z",
     "iopub.status.busy": "2022-02-20T02:29:02.944433Z",
     "iopub.status.idle": "2022-02-20T03:54:33.090243Z",
     "shell.execute_reply": "2022-02-20T03:54:33.089511Z",
     "shell.execute_reply.started": "2022-02-20T02:29:02.944657Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /kaggle/input/gesture-recognition/Project_data/train ; batch size = 20\n",
      "Epoch 1/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.6482 - categorical_accuracy: 0.4618Source path =  /kaggle/input/gesture-recognition/Project_data/val ; batch size = 20\n",
      "34/34 [==============================] - 255s 8s/step - loss: 1.6482 - categorical_accuracy: 0.4618 - val_loss: 5.7820 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00001: saving model to /kaggle/working/models/model_init_2022-02-1911_21_19.152452/model-00001-1.64823-0.46176-5.78204-0.16000.h5\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - 253s 8s/step - loss: 1.1179 - categorical_accuracy: 0.6309 - val_loss: 9.4975 - val_categorical_accuracy: 0.1900\n",
      "\n",
      "Epoch 00002: saving model to /kaggle/working/models/model_init_2022-02-1911_21_19.152452/model-00002-1.11789-0.63088-9.49751-0.19000.h5\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - 257s 8s/step - loss: 0.9169 - categorical_accuracy: 0.6750 - val_loss: 13.3742 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00003: saving model to /kaggle/working/models/model_init_2022-02-1911_21_19.152452/model-00003-0.91693-0.67500-13.37416-0.23000.h5\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - 253s 8s/step - loss: 0.8106 - categorical_accuracy: 0.7324 - val_loss: 15.8663 - val_categorical_accuracy: 0.2400\n",
      "\n",
      "Epoch 00004: saving model to /kaggle/working/models/model_init_2022-02-1911_21_19.152452/model-00004-0.81062-0.73235-15.86628-0.24000.h5\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - 256s 8s/step - loss: 0.5995 - categorical_accuracy: 0.7809 - val_loss: 18.4076 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00005: saving model to /kaggle/working/models/model_init_2022-02-1911_21_19.152452/model-00005-0.59948-0.78088-18.40758-0.21000.h5\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - 254s 8s/step - loss: 0.4089 - categorical_accuracy: 0.8618 - val_loss: 16.3484 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00006: saving model to /kaggle/working/models/model_init_2022-02-1911_21_19.152452/model-00006-0.40889-0.86176-16.34844-0.21000.h5\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - 256s 8s/step - loss: 0.3797 - categorical_accuracy: 0.8662 - val_loss: 10.6810 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00007: saving model to /kaggle/working/models/model_init_2022-02-1911_21_19.152452/model-00007-0.37966-0.86618-10.68097-0.23000.h5\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - 258s 8s/step - loss: 0.2788 - categorical_accuracy: 0.9103 - val_loss: 8.9797 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00008: saving model to /kaggle/working/models/model_init_2022-02-1911_21_19.152452/model-00008-0.27878-0.91029-8.97974-0.21000.h5\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - 257s 8s/step - loss: 0.3072 - categorical_accuracy: 0.8853 - val_loss: 7.6586 - val_categorical_accuracy: 0.2700\n",
      "\n",
      "Epoch 00009: saving model to /kaggle/working/models/model_init_2022-02-1911_21_19.152452/model-00009-0.30721-0.88529-7.65856-0.27000.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - 255s 8s/step - loss: 0.2749 - categorical_accuracy: 0.9118 - val_loss: 6.5133 - val_categorical_accuracy: 0.2800\n",
      "\n",
      "Epoch 00010: saving model to /kaggle/working/models/model_init_2022-02-1911_21_19.152452/model-00010-0.27492-0.91176-6.51326-0.28000.h5\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - 259s 8s/step - loss: 0.2945 - categorical_accuracy: 0.8985 - val_loss: 6.0481 - val_categorical_accuracy: 0.2900\n",
      "\n",
      "Epoch 00011: saving model to /kaggle/working/models/model_init_2022-02-1911_21_19.152452/model-00011-0.29451-0.89853-6.04814-0.29000.h5\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - 254s 8s/step - loss: 0.2299 - categorical_accuracy: 0.9309 - val_loss: 5.2686 - val_categorical_accuracy: 0.3100\n",
      "\n",
      "Epoch 00012: saving model to /kaggle/working/models/model_init_2022-02-1911_21_19.152452/model-00012-0.22993-0.93088-5.26864-0.31000.h5\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - 255s 8s/step - loss: 0.2570 - categorical_accuracy: 0.9235 - val_loss: 4.1998 - val_categorical_accuracy: 0.3700\n",
      "\n",
      "Epoch 00013: saving model to /kaggle/working/models/model_init_2022-02-1911_21_19.152452/model-00013-0.25695-0.92353-4.19978-0.37000.h5\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - 257s 8s/step - loss: 0.2313 - categorical_accuracy: 0.9176 - val_loss: 3.7373 - val_categorical_accuracy: 0.3800\n",
      "\n",
      "Epoch 00014: saving model to /kaggle/working/models/model_init_2022-02-1911_21_19.152452/model-00014-0.23127-0.91765-3.73727-0.38000.h5\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - 256s 8s/step - loss: 0.2191 - categorical_accuracy: 0.9294 - val_loss: 3.1038 - val_categorical_accuracy: 0.3700\n",
      "\n",
      "Epoch 00015: saving model to /kaggle/working/models/model_init_2022-02-1911_21_19.152452/model-00015-0.21911-0.92941-3.10376-0.37000.h5\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - 255s 8s/step - loss: 0.1687 - categorical_accuracy: 0.9500 - val_loss: 2.4872 - val_categorical_accuracy: 0.4600\n",
      "\n",
      "Epoch 00016: saving model to /kaggle/working/models/model_init_2022-02-1911_21_19.152452/model-00016-0.16865-0.95000-2.48718-0.46000.h5\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - 257s 8s/step - loss: 0.2102 - categorical_accuracy: 0.9412 - val_loss: 2.0139 - val_categorical_accuracy: 0.4900\n",
      "\n",
      "Epoch 00017: saving model to /kaggle/working/models/model_init_2022-02-1911_21_19.152452/model-00017-0.21015-0.94118-2.01388-0.49000.h5\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - 255s 8s/step - loss: 0.1620 - categorical_accuracy: 0.9397 - val_loss: 1.5473 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00018: saving model to /kaggle/working/models/model_init_2022-02-1911_21_19.152452/model-00018-0.16204-0.93971-1.54730-0.55000.h5\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - 257s 8s/step - loss: 0.2193 - categorical_accuracy: 0.9412 - val_loss: 1.2859 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00019: saving model to /kaggle/working/models/model_init_2022-02-1911_21_19.152452/model-00019-0.21928-0.94118-1.28586-0.60000.h5\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - 256s 8s/step - loss: 0.2117 - categorical_accuracy: 0.9324 - val_loss: 0.9801 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00020: saving model to /kaggle/working/models/model_init_2022-02-1911_21_19.152452/model-00020-0.21174-0.93235-0.98009-0.65000.h5\n"
     ]
    }
   ],
   "source": [
    "history_3 = train_model(model_3, batch_size = 20, num_epochs = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4 (Conv3D)\n",
    "    batch_size = 20\n",
    "    num_epochs = 30\n",
    "    img_height, img_width = 120, 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T03:54:33.525737Z",
     "iopub.status.busy": "2022-02-20T03:54:33.524233Z",
     "iopub.status.idle": "2022-02-20T03:54:33.530109Z",
     "shell.execute_reply": "2022-02-20T03:54:33.529165Z",
     "shell.execute_reply.started": "2022-02-20T03:54:33.525695Z"
    }
   },
   "outputs": [],
   "source": [
    "num_frames_to_sample = 30\n",
    "img_height, img_width = 120, 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T03:54:33.531776Z",
     "iopub.status.busy": "2022-02-20T03:54:33.531396Z",
     "iopub.status.idle": "2022-02-20T03:54:33.790527Z",
     "shell.execute_reply": "2022-02-20T03:54:33.789801Z",
     "shell.execute_reply.started": "2022-02-20T03:54:33.531732Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_28 (Conv3D)           (None, 30, 120, 120, 16)  1312      \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 30, 120, 120, 16)  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 30, 120, 120, 16)  64        \n",
      "_________________________________________________________________\n",
      "conv3d_29 (Conv3D)           (None, 30, 120, 120, 16)  6928      \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 30, 120, 120, 16)  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 30, 120, 120, 16)  64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_24 (MaxPooling (None, 15, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_30 (Conv3D)           (None, 15, 60, 60, 32)    13856     \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 15, 60, 60, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 15, 60, 60, 32)    128       \n",
      "_________________________________________________________________\n",
      "conv3d_31 (Conv3D)           (None, 15, 60, 60, 32)    27680     \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 15, 60, 60, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 15, 60, 60, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_25 (MaxPooling (None, 7, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_32 (Conv3D)           (None, 7, 30, 30, 64)     55360     \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 7, 30, 30, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 7, 30, 30, 64)     256       \n",
      "_________________________________________________________________\n",
      "conv3d_33 (Conv3D)           (None, 7, 30, 30, 64)     110656    \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 7, 30, 30, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 7, 30, 30, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_26 (MaxPooling (None, 3, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_34 (Conv3D)           (None, 3, 15, 15, 128)    221312    \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 3, 15, 15, 128)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 3, 15, 15, 128)    512       \n",
      "_________________________________________________________________\n",
      "conv3d_35 (Conv3D)           (None, 3, 15, 15, 128)    442496    \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 3, 15, 15, 128)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 3, 15, 15, 128)    512       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_27 (MaxPooling (None, 1, 7, 7, 128)      0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 256)               1605888   \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 2,556,533\n",
      "Trainable params: 2,554,549\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_4 = Sequential()\n",
    "\n",
    "model_4.add(Conv3D(16, (3,3,3), padding='same', input_shape=(num_frames_to_sample, img_height, img_width,3)))\n",
    "model_4.add(Activation('relu'))\n",
    "model_4.add(BatchNormalization())\n",
    "\n",
    "model_4.add(Conv3D(16, (3,3,3), padding='same'))\n",
    "model_4.add(Activation('relu'))\n",
    "model_4.add(BatchNormalization())\n",
    "\n",
    "model_4.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_4.add(Conv3D(32, (3,3,3), padding='same'))\n",
    "model_4.add(Activation('relu'))\n",
    "model_4.add(BatchNormalization())\n",
    "\n",
    "model_4.add(Conv3D(32, (3,3,3), padding='same'))\n",
    "model_4.add(Activation('relu'))\n",
    "model_4.add(BatchNormalization())\n",
    "\n",
    "model_4.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_4.add(Conv3D(64, (3,3,3), padding='same'))\n",
    "model_4.add(Activation('relu'))\n",
    "model_4.add(BatchNormalization())\n",
    "\n",
    "model_4.add(Conv3D(64, (3,3,3), padding='same'))\n",
    "model_4.add(Activation('relu'))\n",
    "model_4.add(BatchNormalization())\n",
    "\n",
    "model_4.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_4.add(Conv3D(128, (3,3,3), padding='same'))\n",
    "model_4.add(Activation('relu'))\n",
    "model_4.add(BatchNormalization())\n",
    "\n",
    "model_4.add(Conv3D(128, (3,3,3), padding='same'))\n",
    "model_4.add(Activation('relu'))\n",
    "model_4.add(BatchNormalization())\n",
    "\n",
    "model_4.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_4.add(Flatten())\n",
    "model_4.add(Dense(256,activation='relu'))\n",
    "model_4.add(BatchNormalization())\n",
    "model_4.add(Dropout(0.5))\n",
    "\n",
    "model_4.add(Dense(256,activation='relu'))\n",
    "model_4.add(BatchNormalization())\n",
    "model_4.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model_4.add(Dense(num_classes,activation='softmax'))\n",
    "\n",
    "optimiser = tf.keras.optimizers.Adam()\n",
    "model_4.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T03:54:33.792375Z",
     "iopub.status.busy": "2022-02-20T03:54:33.792125Z",
     "iopub.status.idle": "2022-02-20T05:41:00.588769Z",
     "shell.execute_reply": "2022-02-20T05:41:00.587349Z",
     "shell.execute_reply.started": "2022-02-20T03:54:33.792340Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /kaggle/input/gesture-recognition/Project_data/train ; batch size = 20\n",
      "Epoch 1/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 2.0917 - categorical_accuracy: 0.3529Source path =  /kaggle/input/gesture-recognition/Project_data/val ; batch size = 20\n",
      "34/34 [==============================] - 214s 6s/step - loss: 2.0917 - categorical_accuracy: 0.3529 - val_loss: 1.6425 - val_categorical_accuracy: 0.3200\n",
      "\n",
      "Epoch 00001: saving model to /kaggle/working/models/model_init_2022-02-1912_45_30.473179/model-00001-2.09175-0.35294-1.64245-0.32000.h5\n",
      "Epoch 2/30\n",
      "34/34 [==============================] - 220s 7s/step - loss: 1.5472 - categorical_accuracy: 0.4588 - val_loss: 3.9207 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00002: saving model to /kaggle/working/models/model_init_2022-02-1912_45_30.473179/model-00002-1.54723-0.45882-3.92065-0.22000.h5\n",
      "Epoch 3/30\n",
      "34/34 [==============================] - 210s 6s/step - loss: 1.4660 - categorical_accuracy: 0.5044 - val_loss: 9.5107 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00003: saving model to /kaggle/working/models/model_init_2022-02-1912_45_30.473179/model-00003-1.46604-0.50441-9.51074-0.16000.h5\n",
      "Epoch 4/30\n",
      "34/34 [==============================] - 213s 6s/step - loss: 1.2259 - categorical_accuracy: 0.5456 - val_loss: 3.7386 - val_categorical_accuracy: 0.2400\n",
      "\n",
      "Epoch 00004: saving model to /kaggle/working/models/model_init_2022-02-1912_45_30.473179/model-00004-1.22588-0.54559-3.73861-0.24000.h5\n",
      "Epoch 5/30\n",
      "34/34 [==============================] - 213s 6s/step - loss: 1.1445 - categorical_accuracy: 0.6044 - val_loss: 2.6529 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00005: saving model to /kaggle/working/models/model_init_2022-02-1912_45_30.473179/model-00005-1.14450-0.60441-2.65287-0.22000.h5\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 6/30\n",
      "34/34 [==============================] - 214s 6s/step - loss: 0.8867 - categorical_accuracy: 0.6926 - val_loss: 3.5784 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00006: saving model to /kaggle/working/models/model_init_2022-02-1912_45_30.473179/model-00006-0.88666-0.69265-3.57840-0.22000.h5\n",
      "Epoch 7/30\n",
      "34/34 [==============================] - 210s 6s/step - loss: 0.8213 - categorical_accuracy: 0.7044 - val_loss: 2.9296 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00007: saving model to /kaggle/working/models/model_init_2022-02-1912_45_30.473179/model-00007-0.82132-0.70441-2.92958-0.22000.h5\n",
      "Epoch 8/30\n",
      "34/34 [==============================] - 212s 6s/step - loss: 0.7238 - categorical_accuracy: 0.7397 - val_loss: 3.0718 - val_categorical_accuracy: 0.2600\n",
      "\n",
      "Epoch 00008: saving model to /kaggle/working/models/model_init_2022-02-1912_45_30.473179/model-00008-0.72381-0.73971-3.07181-0.26000.h5\n",
      "Epoch 9/30\n",
      "34/34 [==============================] - 211s 6s/step - loss: 0.6558 - categorical_accuracy: 0.7588 - val_loss: 3.7263 - val_categorical_accuracy: 0.1900\n",
      "\n",
      "Epoch 00009: saving model to /kaggle/working/models/model_init_2022-02-1912_45_30.473179/model-00009-0.65578-0.75882-3.72630-0.19000.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 10/30\n",
      "34/34 [==============================] - 214s 6s/step - loss: 0.6622 - categorical_accuracy: 0.7471 - val_loss: 3.2335 - val_categorical_accuracy: 0.3100\n",
      "\n",
      "Epoch 00010: saving model to /kaggle/working/models/model_init_2022-02-1912_45_30.473179/model-00010-0.66224-0.74706-3.23346-0.31000.h5\n",
      "Epoch 11/30\n",
      "34/34 [==============================] - 211s 6s/step - loss: 0.5883 - categorical_accuracy: 0.7882 - val_loss: 3.2876 - val_categorical_accuracy: 0.3100\n",
      "\n",
      "Epoch 00011: saving model to /kaggle/working/models/model_init_2022-02-1912_45_30.473179/model-00011-0.58830-0.78824-3.28755-0.31000.h5\n",
      "Epoch 12/30\n",
      "34/34 [==============================] - 214s 6s/step - loss: 0.5709 - categorical_accuracy: 0.8074 - val_loss: 2.5232 - val_categorical_accuracy: 0.3500\n",
      "\n",
      "Epoch 00012: saving model to /kaggle/working/models/model_init_2022-02-1912_45_30.473179/model-00012-0.57091-0.80735-2.52322-0.35000.h5\n",
      "Epoch 13/30\n",
      "34/34 [==============================] - 213s 6s/step - loss: 0.5462 - categorical_accuracy: 0.8000 - val_loss: 1.9908 - val_categorical_accuracy: 0.3400\n",
      "\n",
      "Epoch 00013: saving model to /kaggle/working/models/model_init_2022-02-1912_45_30.473179/model-00013-0.54622-0.80000-1.99083-0.34000.h5\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 14/30\n",
      "34/34 [==============================] - 214s 6s/step - loss: 0.5499 - categorical_accuracy: 0.7926 - val_loss: 1.5215 - val_categorical_accuracy: 0.5400\n",
      "\n",
      "Epoch 00014: saving model to /kaggle/working/models/model_init_2022-02-1912_45_30.473179/model-00014-0.54994-0.79265-1.52151-0.54000.h5\n",
      "Epoch 15/30\n",
      "34/34 [==============================] - 213s 6s/step - loss: 0.5186 - categorical_accuracy: 0.8029 - val_loss: 1.1865 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00015: saving model to /kaggle/working/models/model_init_2022-02-1912_45_30.473179/model-00015-0.51860-0.80294-1.18646-0.60000.h5\n",
      "Epoch 16/30\n",
      "34/34 [==============================] - 212s 6s/step - loss: 0.5118 - categorical_accuracy: 0.8368 - val_loss: 0.9443 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00016: saving model to /kaggle/working/models/model_init_2022-02-1912_45_30.473179/model-00016-0.51184-0.83676-0.94429-0.64000.h5\n",
      "Epoch 17/30\n",
      "34/34 [==============================] - 211s 6s/step - loss: 0.5423 - categorical_accuracy: 0.7941 - val_loss: 0.8136 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00017: saving model to /kaggle/working/models/model_init_2022-02-1912_45_30.473179/model-00017-0.54226-0.79412-0.81359-0.69000.h5\n",
      "Epoch 18/30\n",
      "34/34 [==============================] - 213s 6s/step - loss: 0.5427 - categorical_accuracy: 0.8029 - val_loss: 0.5968 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00018: saving model to /kaggle/working/models/model_init_2022-02-1912_45_30.473179/model-00018-0.54266-0.80294-0.59680-0.76000.h5\n",
      "Epoch 19/30\n",
      "34/34 [==============================] - 212s 6s/step - loss: 0.5395 - categorical_accuracy: 0.8015 - val_loss: 0.5631 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00019: saving model to /kaggle/working/models/model_init_2022-02-1912_45_30.473179/model-00019-0.53948-0.80147-0.56315-0.78000.h5\n",
      "Epoch 20/30\n",
      "34/34 [==============================] - 213s 6s/step - loss: 0.5483 - categorical_accuracy: 0.8000 - val_loss: 0.4962 - val_categorical_accuracy: 0.8400\n",
      "\n",
      "Epoch 00020: saving model to /kaggle/working/models/model_init_2022-02-1912_45_30.473179/model-00020-0.54830-0.80000-0.49621-0.84000.h5\n",
      "Epoch 21/30\n",
      "34/34 [==============================] - 212s 6s/step - loss: 0.5064 - categorical_accuracy: 0.8250 - val_loss: 0.5209 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00021: saving model to /kaggle/working/models/model_init_2022-02-1912_45_30.473179/model-00021-0.50639-0.82500-0.52094-0.83000.h5\n",
      "Epoch 22/30\n",
      "34/34 [==============================] - 211s 6s/step - loss: 0.5222 - categorical_accuracy: 0.8147 - val_loss: 0.4870 - val_categorical_accuracy: 0.8400\n",
      "\n",
      "Epoch 00022: saving model to /kaggle/working/models/model_init_2022-02-1912_45_30.473179/model-00022-0.52219-0.81471-0.48695-0.84000.h5\n",
      "Epoch 23/30\n",
      "34/34 [==============================] - 212s 6s/step - loss: 0.5396 - categorical_accuracy: 0.8221 - val_loss: 0.5072 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00023: saving model to /kaggle/working/models/model_init_2022-02-1912_45_30.473179/model-00023-0.53963-0.82206-0.50717-0.80000.h5\n",
      "Epoch 24/30\n",
      "34/34 [==============================] - 214s 6s/step - loss: 0.4634 - categorical_accuracy: 0.8353 - val_loss: 0.4601 - val_categorical_accuracy: 0.8500\n",
      "\n",
      "Epoch 00024: saving model to /kaggle/working/models/model_init_2022-02-1912_45_30.473179/model-00024-0.46344-0.83529-0.46006-0.85000.h5\n",
      "Epoch 25/30\n",
      "34/34 [==============================] - 212s 6s/step - loss: 0.5203 - categorical_accuracy: 0.8162 - val_loss: 0.5017 - val_categorical_accuracy: 0.8500\n",
      "\n",
      "Epoch 00025: saving model to /kaggle/working/models/model_init_2022-02-1912_45_30.473179/model-00025-0.52026-0.81618-0.50175-0.85000.h5\n",
      "Epoch 26/30\n",
      "34/34 [==============================] - 215s 7s/step - loss: 0.5161 - categorical_accuracy: 0.8103 - val_loss: 0.4554 - val_categorical_accuracy: 0.8500\n",
      "\n",
      "Epoch 00026: saving model to /kaggle/working/models/model_init_2022-02-1912_45_30.473179/model-00026-0.51614-0.81029-0.45544-0.85000.h5\n",
      "Epoch 27/30\n",
      "34/34 [==============================] - 211s 6s/step - loss: 0.5008 - categorical_accuracy: 0.8221 - val_loss: 0.5038 - val_categorical_accuracy: 0.8700\n",
      "\n",
      "Epoch 00027: saving model to /kaggle/working/models/model_init_2022-02-1912_45_30.473179/model-00027-0.50079-0.82206-0.50378-0.87000.h5\n",
      "Epoch 28/30\n",
      "34/34 [==============================] - 212s 6s/step - loss: 0.4536 - categorical_accuracy: 0.8279 - val_loss: 0.3243 - val_categorical_accuracy: 0.8900\n",
      "\n",
      "Epoch 00028: saving model to /kaggle/working/models/model_init_2022-02-1912_45_30.473179/model-00028-0.45359-0.82794-0.32428-0.89000.h5\n",
      "Epoch 29/30\n",
      "34/34 [==============================] - 213s 6s/step - loss: 0.4937 - categorical_accuracy: 0.8250 - val_loss: 0.4927 - val_categorical_accuracy: 0.8500\n",
      "\n",
      "Epoch 00029: saving model to /kaggle/working/models/model_init_2022-02-1912_45_30.473179/model-00029-0.49367-0.82500-0.49275-0.85000.h5\n",
      "Epoch 30/30\n",
      "34/34 [==============================] - 211s 6s/step - loss: 0.4415 - categorical_accuracy: 0.8529 - val_loss: 0.4464 - val_categorical_accuracy: 0.8800\n",
      "\n",
      "Epoch 00030: saving model to /kaggle/working/models/model_init_2022-02-1912_45_30.473179/model-00030-0.44147-0.85294-0.44643-0.88000.h5\n"
     ]
    }
   ],
   "source": [
    "history_4 = train_model(model_4, batch_size = 20, num_epochs = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5 (Conv3D)\n",
    "    batch_size = 20\n",
    "    num_epochs = 30\n",
    "    img_height, img_width = 120, 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T05:41:00.939215Z",
     "iopub.status.busy": "2022-02-20T05:41:00.938767Z",
     "iopub.status.idle": "2022-02-20T05:41:00.942791Z",
     "shell.execute_reply": "2022-02-20T05:41:00.942123Z",
     "shell.execute_reply.started": "2022-02-20T05:41:00.939175Z"
    }
   },
   "outputs": [],
   "source": [
    "num_frames_to_sample = 30\n",
    "img_height, img_width = 120, 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T05:41:00.944965Z",
     "iopub.status.busy": "2022-02-20T05:41:00.944501Z",
     "iopub.status.idle": "2022-02-20T05:41:01.114953Z",
     "shell.execute_reply": "2022-02-20T05:41:01.114228Z",
     "shell.execute_reply.started": "2022-02-20T05:41:00.944930Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_44 (Conv3D)           (None, 30, 120, 120, 16)  1312      \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 30, 120, 120, 16)  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_44 (Batc (None, 30, 120, 120, 16)  64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_36 (MaxPooling (None, 15, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_45 (Conv3D)           (None, 15, 60, 60, 32)    4128      \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 15, 60, 60, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_45 (Batc (None, 15, 60, 60, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_37 (MaxPooling (None, 7, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_46 (Conv3D)           (None, 7, 30, 30, 64)     16448     \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 7, 30, 30, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_46 (Batc (None, 7, 30, 30, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_38 (MaxPooling (None, 3, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_47 (Conv3D)           (None, 3, 15, 15, 128)    65664     \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 3, 15, 15, 128)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_47 (Batc (None, 3, 15, 15, 128)    512       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_39 (MaxPooling (None, 1, 7, 7, 128)      0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "batch_normalization_48 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_49 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 909,637\n",
      "Trainable params: 908,645\n",
      "Non-trainable params: 992\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_5 = Sequential()\n",
    "model_5.add(Conv3D(16, (3, 3, 3), padding='same', input_shape=(num_frames_to_sample, img_height, img_width,3)))\n",
    "model_5.add(Activation('relu'))\n",
    "model_5.add(BatchNormalization())\n",
    "model_5.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_5.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
    "model_5.add(Activation('relu'))\n",
    "model_5.add(BatchNormalization())\n",
    "model_5.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_5.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
    "model_5.add(Activation('relu'))\n",
    "model_5.add(BatchNormalization())\n",
    "model_5.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_5.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
    "model_5.add(Activation('relu'))\n",
    "model_5.add(BatchNormalization())\n",
    "model_5.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_5.add(Flatten())\n",
    "model_5.add(Dense(128,activation='relu'))\n",
    "model_5.add(BatchNormalization())\n",
    "model_5.add(Dropout(0.25))\n",
    "\n",
    "model_5.add(Dense(128,activation='relu'))\n",
    "model_5.add(BatchNormalization())\n",
    "model_5.add(Dropout(0.25))\n",
    "\n",
    "model_5.add(Dense(num_classes,activation='softmax'))\n",
    "\n",
    "optimiser = tf.keras.optimizers.Adam()\n",
    "model_5.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model_5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T05:41:01.116627Z",
     "iopub.status.busy": "2022-02-20T05:41:01.116372Z",
     "iopub.status.idle": "2022-02-20T07:26:29.448616Z",
     "shell.execute_reply": "2022-02-20T07:26:29.447807Z",
     "shell.execute_reply.started": "2022-02-20T05:41:01.116591Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /kaggle/input/gesture-recognition/Project_data/train ; batch size = 20\n",
      "Epoch 1/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.4392 - categorical_accuracy: 0.4750Source path =  /kaggle/input/gesture-recognition/Project_data/val ; batch size = 20\n",
      "34/34 [==============================] - 210s 6s/step - loss: 1.4392 - categorical_accuracy: 0.4750 - val_loss: 3.1507 - val_categorical_accuracy: 0.2400\n",
      "\n",
      "Epoch 00001: saving model to /kaggle/working/models/model_init_2022-02-1914_16_54.364892/model-00001-1.43916-0.47500-3.15071-0.24000.h5\n",
      "Epoch 2/30\n",
      "34/34 [==============================] - 204s 6s/step - loss: 0.8944 - categorical_accuracy: 0.6132 - val_loss: 3.5748 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00002: saving model to /kaggle/working/models/model_init_2022-02-1914_16_54.364892/model-00002-0.89437-0.6132-3.57484-0.21000.h5\n",
      "Epoch 3/30\n",
      "34/34 [==============================] - 209s 6s/step - loss: 0.6422 - categorical_accuracy: 0.6779 - val_loss: 4.2236 - val_categorical_accuracy: 0.1900\n",
      "\n",
      "Epoch 00003: saving model to /kaggle/working/models/model_init_2022-02-1914_16_54.364892/model-00003-0.64215-0.6779-4.22356-0.19000.h5\n",
      "Epoch 4/30\n",
      "34/34 [==============================] - 206s 6s/step - loss: 0.4785 - categorical_accuracy: 0.7279 - val_loss: 4.4963 - val_categorical_accuracy: 0.1100\n",
      "\n",
      "Epoch 00004: saving model to /kaggle/working/models/model_init_2022-02-1914_16_54.364892/model-00004-0.47849-0.7279-4.49627-0.11000.h5\n",
      "Epoch 5/30\n",
      "34/34 [==============================] - 211s 6s/step - loss: 0.3628 - categorical_accuracy: 0.7565 - val_loss: 5.5188 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00005: saving model to /kaggle/working/models/model_init_2022-02-1914_16_54.364892/model-00005-0.36275-0.7565-5.51881-0.16000.h5\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 6/30\n",
      "34/34 [==============================] - 208s 6s/step - loss: 0.2860 - categorical_accuracy: 0.7776 - val_loss: 5.1241 - val_categorical_accuracy: 0.1700\n",
      "\n",
      "Epoch 00006: saving model to /kaggle/working/models/model_init_2022-02-1914_16_54.364892/model-00006-0.28604-0.71765-5.12410-0.17000.h5\n",
      "Epoch 7/30\n",
      "34/34 [==============================] - 210s 6s/step - loss: 0.2127 - categorical_accuracy: 0.8485 - val_loss: 4.4856 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00007: saving model to /kaggle/working/models/model_init_2022-02-1914_16_54.364892/model-00007-0.21273-0.8485-4.48564-0.25000.h5\n",
      "Epoch 8/30\n",
      "34/34 [==============================] - 209s 6s/step - loss: 0.1511 - categorical_accuracy: 0.8632 - val_loss: 4.3333 - val_categorical_accuracy: 0.2600\n",
      "\n",
      "Epoch 00008: saving model to /kaggle/working/models/model_init_2022-02-1914_16_54.364892/model-00008-0.15111-0.8632-4.33334-0.26000.h5\n",
      "Epoch 9/30\n",
      "34/34 [==============================] - 207s 6s/step - loss: 0.1192 - categorical_accuracy: 0.8794 - val_loss: 4.1404 - val_categorical_accuracy: 0.2800\n",
      "\n",
      "Epoch 00009: saving model to /kaggle/working/models/model_init_2022-02-1914_16_54.364892/model-00009-0.11915-0.8794-4.14040-0.28000.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 10/30\n",
      "34/34 [==============================] - 209s 6s/step - loss: 0.1381 - categorical_accuracy: 0.8662 - val_loss: 4.2086 - val_categorical_accuracy: 0.3100\n",
      "\n",
      "Epoch 00010: saving model to /kaggle/working/models/model_init_2022-02-1914_16_54.364892/model-00010-0.13815-0.8662-4.20865-0.31000.h5\n",
      "Epoch 11/30\n",
      "34/34 [==============================] - 211s 6s/step - loss: 0.1427 - categorical_accuracy: 0.8676 - val_loss: 4.4644 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00011: saving model to /kaggle/working/models/model_init_2022-02-1914_16_54.364892/model-00011-0.14273-0.86765-4.46443-0.30000.h5\n",
      "Epoch 12/30\n",
      "34/34 [==============================] - 211s 6s/step - loss: 0.1028 - categorical_accuracy: 0.8838 - val_loss: 3.7871 - val_categorical_accuracy: 0.3300\n",
      "\n",
      "Epoch 00012: saving model to /kaggle/working/models/model_init_2022-02-1914_16_54.364892/model-00012-0.10278-0.88382-3.78706-0.33000.h5\n",
      "Epoch 13/30\n",
      "34/34 [==============================] - 208s 6s/step - loss: 0.1184 - categorical_accuracy: 0.8750 - val_loss: 3.3249 - val_categorical_accuracy: 0.3400\n",
      "\n",
      "Epoch 00013: saving model to /kaggle/working/models/model_init_2022-02-1914_16_54.364892/model-00013-0.11845-0.87500-3.32490-0.34000.h5\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 14/30\n",
      "34/34 [==============================] - 208s 6s/step - loss: 0.1113 - categorical_accuracy: 0.9065 - val_loss: 2.5954 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00014: saving model to /kaggle/working/models/model_init_2022-02-1914_16_54.364892/model-00014-0.11131-0.90647-2.59539-0.40000.h5\n",
      "Epoch 15/30\n",
      "34/34 [==============================] - 209s 6s/step - loss: 0.1236 - categorical_accuracy: 0.92794 - val_loss: 1.9815 - val_categorical_accuracy: 0.4800\n",
      "\n",
      "Epoch 00015: saving model to /kaggle/working/models/model_init_2022-02-1914_16_54.364892/model-00015-0.12364-0.92794-1.98146-0.48000.h5\n",
      "Epoch 16/30\n",
      "34/34 [==============================] - 210s 6s/step - loss: 0.1140 - categorical_accuracy: 0.9379 - val_loss: 1.4006 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00016: saving model to /kaggle/working/models/model_init_2022-02-1914_16_54.364892/model-00016-0.11402-0.93794-1.40063-0.51000.h5\n",
      "Epoch 17/30\n",
      "34/34 [==============================] - 208s 6s/step - loss: 0.1105 - categorical_accuracy: 0.9279 - val_loss: 1.0119 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00017: saving model to /kaggle/working/models/model_init_2022-02-1914_16_54.364892/model-00017-0.11052-0.92794-1.01193-0.67000.h5\n",
      "Epoch 18/30\n",
      "34/34 [==============================] - 209s 6s/step - loss: 0.1096 - categorical_accuracy: 0.9594 - val_loss: 0.7756 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00018: saving model to /kaggle/working/models/model_init_2022-02-1914_16_54.364892/model-00018-0.10963-0.95941-0.77559-0.71000.h5\n",
      "Epoch 19/30\n",
      "34/34 [==============================] - 207s 6s/step - loss: 0.0951 - categorical_accuracy: 0.9468 - val_loss: 0.7473 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00019: saving model to /kaggle/working/models/model_init_2022-02-1914_16_54.364892/model-00019-0.09514-0.94676-0.74730-0.76000.h5\n",
      "Epoch 20/30\n",
      "34/34 [==============================] - 210s 6s/step - loss: 0.1241 - categorical_accuracy: 0.9765 - val_loss: 0.6630 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00020: saving model to /kaggle/working/models/model_init_2022-02-1914_16_54.364892/model-00020-0.12409-0.97647-0.66297-0.82000.h5\n",
      "Epoch 21/30\n",
      "34/34 [==============================] - 208s 6s/step - loss: 0.1004 - categorical_accuracy: 0.9794 - val_loss: 0.5904 - val_categorical_accuracy: 0.8500\n",
      "\n",
      "Epoch 00021: saving model to /kaggle/working/models/model_init_2022-02-1914_16_54.364892/model-00021-0.10038-0.97941-0.59035-0.85000.h5\n",
      "Epoch 22/30\n",
      "34/34 [==============================] - 210s 6s/step - loss: 0.0924 - categorical_accuracy: 0.9853 - val_loss: 0.4905 - val_categorical_accuracy: 0.8700\n",
      "\n",
      "Epoch 00022: saving model to /kaggle/working/models/model_init_2022-02-1914_16_54.364892/model-00022-0.09245-0.98529-0.49046-0.87000.h5\n",
      "Epoch 23/30\n",
      "34/34 [==============================] - 207s 6s/step - loss: 0.1114 - categorical_accuracy: 0.9853 - val_loss: 0.4508 - val_categorical_accuracy: 0.8900\n",
      "\n",
      "Epoch 00023: saving model to /kaggle/working/models/model_init_2022-02-1914_16_54.364892/model-00023-0.11143-0.98529-0.45080-0.89000.h5\n",
      "Epoch 24/30\n",
      "34/34 [==============================] - 213s 6s/step - loss: 0.0857 - categorical_accuracy: 0.9779 - val_loss: 0.6065 - val_categorical_accuracy: 0.8500\n",
      "\n",
      "Epoch 00024: saving model to /kaggle/working/models/model_init_2022-02-1914_16_54.364892/model-00024-0.08574-0.97794-0.60654-0.85000.h5\n",
      "Epoch 25/30\n",
      "34/34 [==============================] - 214s 6s/step - loss: 0.0999 - categorical_accuracy: 0.9794 - val_loss: 0.4791 - val_categorical_accuracy: 0.9000\n",
      "\n",
      "Epoch 00025: saving model to /kaggle/working/models/model_init_2022-02-1914_16_54.364892/model-00025-0.09987-0.97941-0.47911-0.90000.h5\n",
      "Epoch 26/30\n",
      "34/34 [==============================] - 213s 6s/step - loss: 0.0902 - categorical_accuracy: 0.9738 - val_loss: 0.5062 - val_categorical_accuracy: 0.9310\n",
      "\n",
      "Epoch 00026: saving model to /kaggle/working/models/model_init_2022-02-1914_16_54.364892/model-00026-0.09021-0.97382-0.50617-0.93100.h5\n",
      "Epoch 27/30\n",
      "34/34 [==============================] - 213s 6s/step - loss: 0.0923 - categorical_accuracy: 0.9653 - val_loss: 0.4602 - val_categorical_accuracy: 0.9100\n",
      "\n",
      "Epoch 00027: saving model to /kaggle/working/models/model_init_2022-02-1914_16_54.364892/model-00027-0.09229-0.96529-0.46015-0.91000.h5\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 28/30\n",
      "34/34 [==============================] - 211s 6s/step - loss: 0.1167 - categorical_accuracy: 0.9750 - val_loss: 0.4514 - val_categorical_accuracy: 0.8700\n",
      "34/34 [==============================] - 211s 6s/step - loss: 0.0834 - categorical_accuracy: 0.9768 - val_loss: 0.5306 - val_categorical_accuracy: 0.8400\n",
      "\n",
      "Epoch 00029: saving model to /kaggle/working/models/model_init_2022-02-1914_16_54.364892/model-00029-0.08343-0.97676-0.53058-0.84000.h5\n",
      "Epoch 30/30\n",
      "34/34 [==============================] - 211s 6s/step - loss: 0.1147 - categorical_accuracy: 0.9653 - val_loss: 0.4678 - val_categorical_accuracy: 0.9200\n",
      "\n",
      "Epoch 00030: saving model to /kaggle/working/models/model_init_2022-02-1914_16_54.364892/model-00030-0.11465-0.96529-0.46781-0.92000.h5\n"
     ]
    }
   ],
   "source": [
    "history_5 = train_model(model_5, batch_size = 20, num_epochs = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 6 (Conv3D)\n",
    "    batch_size = 30\n",
    "    num_epochs = 20\n",
    "    img_height, img_width = 120, 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T07:26:29.809681Z",
     "iopub.status.busy": "2022-02-20T07:26:29.809258Z",
     "iopub.status.idle": "2022-02-20T07:26:29.814018Z",
     "shell.execute_reply": "2022-02-20T07:26:29.813023Z",
     "shell.execute_reply.started": "2022-02-20T07:26:29.809642Z"
    }
   },
   "outputs": [],
   "source": [
    "num_frames_to_sample = 30\n",
    "img_height, img_width = 120, 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T07:26:29.815629Z",
     "iopub.status.busy": "2022-02-20T07:26:29.815315Z",
     "iopub.status.idle": "2022-02-20T07:26:29.986400Z",
     "shell.execute_reply": "2022-02-20T07:26:29.985634Z",
     "shell.execute_reply.started": "2022-02-20T07:26:29.815594Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_60 (Conv3D)           (None, 30, 120, 120, 16)  1312      \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 30, 120, 120, 16)  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_68 (Batc (None, 30, 120, 120, 16)  64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_52 (MaxPooling (None, 15, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_61 (Conv3D)           (None, 15, 60, 60, 32)    13856     \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 15, 60, 60, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_69 (Batc (None, 15, 60, 60, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_53 (MaxPooling (None, 7, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_62 (Conv3D)           (None, 7, 30, 30, 64)     16448     \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 7, 30, 30, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_70 (Batc (None, 7, 30, 30, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_54 (MaxPooling (None, 3, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_63 (Conv3D)           (None, 3, 15, 15, 128)    65664     \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 3, 15, 15, 128)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_71 (Batc (None, 3, 15, 15, 128)    512       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_55 (MaxPooling (None, 1, 7, 7, 128)      0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 64)                401472    \n",
      "_________________________________________________________________\n",
      "batch_normalization_72 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_73 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 504,709\n",
      "Trainable params: 503,973\n",
      "Non-trainable params: 736\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_6 = Sequential()\n",
    "model_6.add(Conv3D(16, (3, 3, 3), padding='same', input_shape=(num_frames_to_sample, img_height,img_width,3)))\n",
    "model_6.add(Activation('relu'))\n",
    "model_6.add(BatchNormalization())\n",
    "model_6.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_6.add(Conv3D(32, (3, 3, 3), padding='same'))\n",
    "model_6.add(Activation('relu'))\n",
    "model_6.add(BatchNormalization())\n",
    "model_6.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_6.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
    "model_6.add(Activation('relu'))\n",
    "model_6.add(BatchNormalization())\n",
    "model_6.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_6.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
    "model_6.add(Activation('relu'))\n",
    "model_6.add(BatchNormalization())\n",
    "model_6.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_6.add(Flatten())\n",
    "model_6.add(Dense(64,activation='relu'))\n",
    "model_6.add(BatchNormalization())\n",
    "model_6.add(Dropout(0.3))\n",
    "\n",
    "model_6.add(Dense(64,activation='relu'))\n",
    "model_6.add(BatchNormalization())\n",
    "model_6.add(Dropout(0.3))\n",
    "\n",
    "model_6.add(Dense(num_classes,activation='softmax'))\n",
    "\n",
    "optimiser = tf.keras.optimizers.Adam()\n",
    "model_6.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model_6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T07:26:29.987860Z",
     "iopub.status.busy": "2022-02-20T07:26:29.987618Z",
     "iopub.status.idle": "2022-02-20T07:26:37.593765Z",
     "shell.execute_reply": "2022-02-20T07:26:37.590285Z",
     "shell.execute_reply.started": "2022-02-20T07:26:29.987825Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /kaggle/input/gesture-recognition/Project_data/train ; batch size = 20\n",
      "Epoch 1/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.6071 - categorical_accuracy: 0.3971Source path =  /kaggle/input/gesture-recognition/Project_data/val ; batch size = 20\n",
      "34/34 [==============================] - 215s 6s/step - loss: 1.6071 - categorical_accuracy: 0.3971 - val_loss: 2.4000 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00001: saving model to /kaggle/working/models/model_init_2022-02-1916_56_53.421991/model-00001-1.60714-0.39706-2.39998-0.20000.h5\n",
      "Epoch 2/25\n",
      "34/34 [==============================] - 209s 6s/step - loss: 1.0896 - categorical_accuracy: 0.5662 - val_loss: 4.3015 - val_categorical_accuracy: 0.1800\n",
      "\n",
      "Epoch 00002: saving model to /kaggle/working/models/model_init_2022-02-1916_56_53.421991/model-00002-1.08961-0.56618-4.30147-0.18000.h5\n",
      "Epoch 3/25\n",
      "34/34 [==============================] - 208s 6s/step - loss: 0.8052 - categorical_accuracy: 0.6971 - val_loss: 4.5880 - val_categorical_accuracy: 0.1800\n",
      "\n",
      "Epoch 00003: saving model to /kaggle/working/models/model_init_2022-02-1916_56_53.421991/model-00003-0.80520-0.69706-4.58797-0.18000.h5\n",
      "Epoch 4/25\n",
      "34/34 [==============================] - 208s 6s/step - loss: 0.6792 - categorical_accuracy: 0.7529 - val_loss: 4.3534 - val_categorical_accuracy: 0.2900\n",
      "\n",
      "Epoch 00004: saving model to /kaggle/working/models/model_init_2022-02-1916_56_53.421991/model-00004-0.67918-0.75294-4.35339-0.29000.h5\n",
      "Epoch 5/25\n",
      "34/34 [==============================] - 208s 6s/step - loss: 0.5320 - categorical_accuracy: 0.8000 - val_loss: 5.2106 - val_categorical_accuracy: 0.1700\n",
      "\n",
      "Epoch 00005: saving model to /kaggle/working/models/model_init_2022-02-1916_56_53.421991/model-00005-0.53200-0.80000-5.21061-0.17000.h5\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 6/25\n",
      "34/34 [==============================] - 209s 6s/step - loss: 0.4327 - categorical_accuracy: 0.8574 - val_loss: 4.9850 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00006: saving model to /kaggle/working/models/model_init_2022-02-1916_56_53.421991/model-00006-0.43271-0.85735-4.98499-0.16000.h5\n",
      "Epoch 7/25\n",
      "34/34 [==============================] - 208s 6s/step - loss: 0.3893 - categorical_accuracy: 0.8662 - val_loss: 4.3010 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00007: saving model to /kaggle/working/models/model_init_2022-02-1916_56_53.421991/model-00007-0.38927-0.86618-4.30101-0.20000.h5\n",
      "Epoch 8/25\n",
      "34/34 [==============================] - 208s 6s/step - loss: 0.3545 - categorical_accuracy: 0.8882 - val_loss: 4.1147 - val_categorical_accuracy: 0.1800\n",
      "\n",
      "Epoch 00008: saving model to /kaggle/working/models/model_init_2022-02-1916_56_53.421991/model-00008-0.35450-0.88824-4.11475-0.18000.h5\n",
      "Epoch 9/25\n",
      "34/34 [==============================] - 206s 6s/step - loss: 0.3568 - categorical_accuracy: 0.8897 - val_loss: 3.1958 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00009: saving model to /kaggle/working/models/model_init_2022-02-1916_56_53.421991/model-00009-0.35679-0.88971-3.19583-0.25000.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 10/25\n",
      "34/34 [==============================] - 210s 6s/step - loss: 0.2918 - categorical_accuracy: 0.9074 - val_loss: 3.3722 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00010: saving model to /kaggle/working/models/model_init_2022-02-1916_56_53.421991/model-00010-0.29184-0.90735-3.37218-0.23000.h5\n",
      "Epoch 11/25\n",
      "34/34 [==============================] - 207s 6s/step - loss: 0.2874 - categorical_accuracy: 0.9147 - val_loss: 2.6978 - val_categorical_accuracy: 0.2900\n",
      "\n",
      "Epoch 00011: saving model to /kaggle/working/models/model_init_2022-02-1916_56_53.421991/model-00011-0.28740-0.91471-2.69777-0.29000.h5\n",
      "Epoch 12/25\n",
      "34/34 [==============================] - 210s 6s/step - loss: 0.3129 - categorical_accuracy: 0.9206 - val_loss: 2.2639 - val_categorical_accuracy: 0.3400\n",
      "\n",
      "Epoch 00012: saving model to /kaggle/working/models/model_init_2022-02-1916_56_53.421991/model-00012-0.31294-0.92059-2.26394-0.34000.h5\n",
      "Epoch 13/25\n",
      "34/34 [==============================] - 206s 6s/step - loss: 0.2627 - categorical_accuracy: 0.9250 - val_loss: 1.9682 - val_categorical_accuracy: 0.3200\n",
      "\n",
      "Epoch 00013: saving model to /kaggle/working/models/model_init_2022-02-1916_56_53.421991/model-00013-0.26268-0.92500-1.96823-0.32000.h5\n",
      "Epoch 14/25\n",
      "34/34 [==============================] - 206s 6s/step - loss: 0.2645 - categorical_accuracy: 0.9206 - val_loss: 1.6197 - val_categorical_accuracy: 0.3500\n",
      "\n",
      "Epoch 00014: saving model to /kaggle/working/models/model_init_2022-02-1916_56_53.421991/model-00014-0.26446-0.92059-1.61973-0.35000.h5\n",
      "Epoch 15/25\n",
      "34/34 [==============================] - 209s 6s/step - loss: 0.2497 - categorical_accuracy: 0.9353 - val_loss: 1.2102 - val_categorical_accuracy: 0.5300\n",
      "\n",
      "Epoch 00015: saving model to /kaggle/working/models/model_init_2022-02-1916_56_53.421991/model-00015-0.24965-0.93529-1.21020-0.53000.h5\n",
      "Epoch 16/25\n",
      "34/34 [==============================] - 205s 6s/step - loss: 0.2136 - categorical_accuracy: 0.9456 - val_loss: 1.1021 - val_categorical_accuracy: 0.5300\n",
      "\n",
      "Epoch 00016: saving model to /kaggle/working/models/model_init_2022-02-1916_56_53.421991/model-00016-0.21360-0.94559-1.10211-0.53000.h5\n",
      "Epoch 17/25\n",
      "34/34 [==============================] - 207s 6s/step - loss: 0.2164 - categorical_accuracy: 0.9338 - val_loss: 0.8467 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00017: saving model to /kaggle/working/models/model_init_2022-02-1916_56_53.421991/model-00017-0.21640-0.93382-0.84670-0.64000.h5\n",
      "Epoch 18/25\n",
      "34/34 [==============================] - 210s 6s/step - loss: 0.2247 - categorical_accuracy: 0.9529 - val_loss: 0.8083 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00018: saving model to /kaggle/working/models/model_init_2022-02-1916_56_53.421991/model-00018-0.22469-0.95294-0.80830-0.70000.h5\n",
      "Epoch 19/25\n",
      "34/34 [==============================] - 206s 6s/step - loss: 0.2272 - categorical_accuracy: 0.9441 - val_loss: 0.6653 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00019: saving model to /kaggle/working/models/model_init_2022-02-1916_56_53.421991/model-00019-0.22719-0.94412-0.66532-0.69000.h5\n",
      "Epoch 20/25\n",
      "34/34 [==============================] - 207s 6s/step - loss: 0.2074 - categorical_accuracy: 0.9456 - val_loss: 0.5886 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00020: saving model to /kaggle/working/models/model_init_2022-02-1916_56_53.421991/model-00020-0.20741-0.94559-0.58860-0.79000.h5\n",
      "Epoch 21/25\n",
      "34/34 [==============================] - 207s 6s/step - loss: 0.2342 - categorical_accuracy: 0.9382 - val_loss: 0.5640 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00021: saving model to /kaggle/working/models/model_init_2022-02-1916_56_53.421991/model-00021-0.23417-0.93824-0.56397-0.80000.h5\n",
      "Epoch 22/25\n",
      "34/34 [==============================] - 208s 6s/step - loss: 0.2254 - categorical_accuracy: 0.9544 - val_loss: 0.4240 - val_categorical_accuracy: 0.8400\n",
      "\n",
      "Epoch 00022: saving model to /kaggle/working/models/model_init_2022-02-1916_56_53.421991/model-00022-0.22540-0.95441-0.42397-0.84000.h5\n",
      "Epoch 23/25\n",
      "34/34 [==============================] - 208s 6s/step - loss: 0.2437 - categorical_accuracy: 0.9368 - val_loss: 0.3120 - val_categorical_accuracy: 0.9100\n",
      "\n",
      "Epoch 00023: saving model to /kaggle/working/models/model_init_2022-02-1916_56_53.421991/model-00023-0.24372-0.93676-0.31204-0.91000.h5\n",
      "Epoch 24/25\n",
      "34/34 [==============================] - 205s 6s/step - loss: 0.2039 - categorical_accuracy: 0.9500 - val_loss: 0.4906 - val_categorical_accuracy: 0.8500\n",
      "\n",
      "Epoch 00024: saving model to /kaggle/working/models/model_init_2022-02-1916_56_53.421991/model-00024-0.20386-0.95000-0.49060-0.85000.h5\n",
      "Epoch 25/25\n",
      "34/34 [==============================] - 208s 6s/step - loss: 0.1925 - categorical_accuracy: 0.9544 - val_loss: 0.4256 - val_categorical_accuracy: 0.8700\n",
      "\n",
      "Epoch 00025: saving model to /kaggle/working/models/model_init_2022-02-1916_56_53.421991/model-00025-0.19249-0.95441-0.42558-0.87000.h5\n"
     ]
    }
   ],
   "source": [
    "history_6 = train_model(model_6, batch_size = 20, num_epochs = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: using CNN + RNN\n",
    "\n",
    "### Model 7 (CNN + SimpleRNN)\n",
    "    batch_size = 10\n",
    "    num_epochs = 10\n",
    "    img_height, img_width = 120, 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T07:29:16.720429Z",
     "iopub.status.busy": "2022-02-20T07:29:16.719582Z",
     "iopub.status.idle": "2022-02-20T07:29:16.725743Z",
     "shell.execute_reply": "2022-02-20T07:29:16.724778Z",
     "shell.execute_reply.started": "2022-02-20T07:29:16.720395Z"
    }
   },
   "outputs": [],
   "source": [
    "num_frames_to_sample = 30\n",
    "img_height, img_width = 120, 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T07:29:16.727986Z",
     "iopub.status.busy": "2022-02-20T07:29:16.727217Z",
     "iopub.status.idle": "2022-02-20T07:29:17.170867Z",
     "shell.execute_reply": "2022-02-20T07:29:17.170130Z",
     "shell.execute_reply.started": "2022-02-20T07:29:16.727949Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed (TimeDistri (None, 30, 120, 120, 16)  448       \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 30, 120, 120, 16)  64        \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 30, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 30, 60, 60, 32)    4640      \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 30, 60, 60, 32)    128       \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 30, 30, 30, 32)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 30, 30, 30, 64)    18496     \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, 30, 30, 30, 64)    256       \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 30, 15, 15, 64)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_9 (TimeDist (None, 30, 15, 15, 128)   73856     \n",
      "_________________________________________________________________\n",
      "time_distributed_10 (TimeDis (None, 30, 15, 15, 128)   512       \n",
      "_________________________________________________________________\n",
      "time_distributed_11 (TimeDis (None, 30, 7, 7, 128)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_12 (TimeDis (None, 30, 6272)          0         \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 256)               1671424   \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 1,836,901\n",
      "Trainable params: 1,836,421\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_7 = Sequential()\n",
    "\n",
    "model_7.add(TimeDistributed(Conv2D(16, (3, 3) , padding='same', activation='relu'), input_shape = (num_frames_to_sample, img_height, img_width, 3)))\n",
    "model_7.add(TimeDistributed(BatchNormalization()))\n",
    "model_7.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "model_7.add(TimeDistributed(Conv2D(32, (3, 3) , padding='same', activation='relu')))\n",
    "model_7.add(TimeDistributed(BatchNormalization()))\n",
    "model_7.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "model_7.add(TimeDistributed(Conv2D(64, (3, 3) , padding='same', activation='relu')))\n",
    "model_7.add(TimeDistributed(BatchNormalization()))\n",
    "model_7.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "model_7.add(TimeDistributed(Conv2D(128, (3, 3) , padding='same', activation='relu')))\n",
    "model_7.add(TimeDistributed(BatchNormalization()))\n",
    "model_7.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "model_7.add(TimeDistributed(Flatten()))\n",
    "\n",
    "\n",
    "model_7.add(SimpleRNN(256))\n",
    "\n",
    "model_7.add(Dense(256,activation='relu'))\n",
    "model_7.add(Dropout(0.3))\n",
    "\n",
    "model_7.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "optimiser = tf.keras.optimizers.Adam()\n",
    "model_7.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model_7.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T07:29:17.173272Z",
     "iopub.status.busy": "2022-02-20T07:29:17.172930Z",
     "iopub.status.idle": "2022-02-20T08:42:07.062999Z",
     "shell.execute_reply": "2022-02-20T08:42:07.061546Z",
     "shell.execute_reply.started": "2022-02-20T07:29:17.173230Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /kaggle/input/gesture-recognition/Project_data/train ; batch size = 10\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.4132 - categorical_accuracy: 0.4224Source path =  /kaggle/input/gesture-recognition/Project_data/val ; batch size = 10\n",
      "67/67 [==============================] - 308s 5s/step - loss: 1.4132 - categorical_accuracy: 0.4224 - val_loss: 4.5476 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00001: saving model to /kaggle/working/models/model_init_2022-02-2001_02_11.519018/model-00001-1.41318-0.42239-4.54758-0.21000.h5\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 249s 4s/step - loss: 1.0478 - categorical_accuracy: 0.5716 - val_loss: 4.8890 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00002: saving model to /kaggle/working/models/model_init_2022-02-2001_02_11.519018/model-00002-1.04784-0.57164-4.88897-0.22000.h5\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 210s 3s/step - loss: 1.0298 - categorical_accuracy: 0.5925 - val_loss: 3.0285 - val_categorical_accuracy: 0.3200\n",
      "\n",
      "Epoch 00003: saving model to /kaggle/working/models/model_init_2022-02-2001_02_11.519018/model-00003-1.02983-0.59254-3.02852-0.32000.h5\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 209s 3s/step - loss: 0.9098 - categorical_accuracy: 0.6507 - val_loss: 2.8863 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00004: saving model to /kaggle/working/models/model_init_2022-02-2001_02_11.519018/model-00004-0.90981-0.65075-2.88633-0.23000.h5\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 208s 3s/step - loss: 0.7599 - categorical_accuracy: 0.7149 - val_loss: 2.5938 - val_categorical_accuracy: 0.2600\n",
      "\n",
      "Epoch 00005: saving model to /kaggle/working/models/model_init_2022-02-2001_02_11.519018/model-00005-0.75990-0.71493-2.59383-0.26000.h5\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 209s 3s/step - loss: 0.7044 - categorical_accuracy: 0.7343 - val_loss: 2.1110 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00006: saving model to /kaggle/working/models/model_init_2022-02-2001_02_11.519018/model-00006-0.70440-0.73433-2.11098-0.40000.h5\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 206s 3s/step - loss: 0.7221 - categorical_accuracy: 0.7328 - val_loss: 1.6135 - val_categorical_accuracy: 0.4300\n",
      "\n",
      "Epoch 00007: saving model to /kaggle/working/models/model_init_2022-02-2001_02_11.519018/model-00007-0.72207-0.73284-1.61352-0.43000.h5\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 209s 3s/step - loss: 0.5476 - categorical_accuracy: 0.7851 - val_loss: 1.1111 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00008: saving model to /kaggle/working/models/model_init_2022-02-2001_02_11.519018/model-00008-0.54757-0.78507-1.11111-0.63000.h5\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 209s 3s/step - loss: 0.4843 - categorical_accuracy: 0.8269 - val_loss: 0.9384 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00009: saving model to /kaggle/working/models/model_init_2022-02-2001_02_11.519018/model-00009-0.48428-0.82687-0.93840-0.69000.h5\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 210s 3s/step - loss: 0.4719 - categorical_accuracy: 0.8239 - val_loss: 0.8690 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00010: saving model to /kaggle/working/models/model_init_2022-02-2001_02_11.519018/model-00010-0.47189-0.82388-0.86900-0.70000.h5\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 209s 3s/step - loss: 0.4878 - categorical_accuracy: 0.8313 - val_loss: 1.0041 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00011: saving model to /kaggle/working/models/model_init_2022-02-2001_02_11.519018/model-00011-0.48781-0.83134-1.00415-0.65000.h5\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 209s 3s/step - loss: 0.4076 - categorical_accuracy: 0.8567 - val_loss: 0.8057 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00012: saving model to /kaggle/working/models/model_init_2022-02-2001_02_11.519018/model-00012-0.40764-0.85672-0.80567-0.72000.h5\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 229s 3s/step - loss: 0.4175 - categorical_accuracy: 0.8537 - val_loss: 0.7493 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00013: saving model to /kaggle/working/models/model_init_2022-02-2001_02_11.519018/model-00013-0.41745-0.85373-0.74929-0.74000.h5\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 219s 3s/step - loss: 0.4079 - categorical_accuracy: 0.8493 - val_loss: 1.5442 - val_categorical_accuracy: 0.5900\n",
      "\n",
      "Epoch 00014: saving model to /kaggle/working/models/model_init_2022-02-2001_02_11.519018/model-00014-0.40791-0.84925-1.54423-0.59000.h5\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 216s 3s/step - loss: 0.3240 - categorical_accuracy: 0.8955 - val_loss: 1.1847 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00015: saving model to /kaggle/working/models/model_init_2022-02-2001_02_11.519018/model-00015-0.32401-0.89552-1.18466-0.63000.h5\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 212s 3s/step - loss: 0.2259 - categorical_accuracy: 0.9224 - val_loss: 0.6077 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00016: saving model to /kaggle/working/models/model_init_2022-02-2001_02_11.519018/model-00016-0.22591-0.92239-0.60772-0.76000.h5\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 210s 3s/step - loss: 0.2416 - categorical_accuracy: 0.9209 - val_loss: 1.3260 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00017: saving model to /kaggle/working/models/model_init_2022-02-2001_02_11.519018/model-00017-0.24158-0.92090-1.32602-0.66000.h5\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 210s 3s/step - loss: 0.2598 - categorical_accuracy: 0.9194 - val_loss: 0.9107 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00018: saving model to /kaggle/working/models/model_init_2022-02-2001_02_11.519018/model-00018-0.25982-0.91940-0.91071-0.75000.h5\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 212s 3s/step - loss: 0.2656 - categorical_accuracy: 0.9000 - val_loss: 0.8618 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00019: saving model to /kaggle/working/models/model_init_2022-02-2001_02_11.519018/model-00019-0.26560-0.90000-0.86182-0.71000.h5\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 209s 3s/step - loss: 0.2347 - categorical_accuracy: 0.9254 - val_loss: 0.9658 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00020: saving model to /kaggle/working/models/model_init_2022-02-2001_02_11.519018/model-00020-0.23468-0.92537-0.96578-0.72000.h5\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n"
     ]
    }
   ],
   "source": [
    "history_7 = train_model(model_7, batch_size = 10, num_epochs = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 8 (CNN + LSTM)\n",
    "    batch_size = 10\n",
    "    num_epochs = 10\n",
    "    img_height, img_width = 120, 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T08:42:07.506395Z",
     "iopub.status.busy": "2022-02-20T08:42:07.505937Z",
     "iopub.status.idle": "2022-02-20T08:42:07.510718Z",
     "shell.execute_reply": "2022-02-20T08:42:07.509677Z",
     "shell.execute_reply.started": "2022-02-20T08:42:07.506357Z"
    }
   },
   "outputs": [],
   "source": [
    "num_frames_to_sample = 30\n",
    "img_height, img_width = 120, 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T08:42:07.512806Z",
     "iopub.status.busy": "2022-02-20T08:42:07.512220Z",
     "iopub.status.idle": "2022-02-20T08:42:07.812765Z",
     "shell.execute_reply": "2022-02-20T08:42:07.811971Z",
     "shell.execute_reply.started": "2022-02-20T08:42:07.512770Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_13 (TimeDis (None, 30, 120, 120, 16)  448       \n",
      "_________________________________________________________________\n",
      "time_distributed_14 (TimeDis (None, 30, 120, 120, 16)  64        \n",
      "_________________________________________________________________\n",
      "time_distributed_15 (TimeDis (None, 30, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_16 (TimeDis (None, 30, 60, 60, 32)    4640      \n",
      "_________________________________________________________________\n",
      "time_distributed_17 (TimeDis (None, 30, 60, 60, 32)    128       \n",
      "_________________________________________________________________\n",
      "time_distributed_18 (TimeDis (None, 30, 30, 30, 32)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_19 (TimeDis (None, 30, 30, 30, 64)    18496     \n",
      "_________________________________________________________________\n",
      "time_distributed_20 (TimeDis (None, 30, 30, 30, 64)    256       \n",
      "_________________________________________________________________\n",
      "time_distributed_21 (TimeDis (None, 30, 15, 15, 64)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_22 (TimeDis (None, 30, 15, 15, 128)   73856     \n",
      "_________________________________________________________________\n",
      "time_distributed_23 (TimeDis (None, 30, 15, 15, 128)   512       \n",
      "_________________________________________________________________\n",
      "time_distributed_24 (TimeDis (None, 30, 7, 7, 128)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_25 (TimeDis (None, 30, 7, 7, 256)     295168    \n",
      "_________________________________________________________________\n",
      "time_distributed_26 (TimeDis (None, 30, 7, 7, 256)     1024      \n",
      "_________________________________________________________________\n",
      "time_distributed_27 (TimeDis (None, 30, 3, 3, 256)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_28 (TimeDis (None, 30, 2304)          0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               1245696   \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 1,657,445\n",
      "Trainable params: 1,656,453\n",
      "Non-trainable params: 992\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_8 = Sequential()\n",
    "\n",
    "model_8.add(TimeDistributed(Conv2D(16, (3, 3) , padding='same', activation='relu'), input_shape = (num_frames_to_sample, img_height, img_width, 3)))\n",
    "model_8.add(TimeDistributed(BatchNormalization()))\n",
    "model_8.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "model_8.add(TimeDistributed(Conv2D(32, (3, 3) , padding='same', activation='relu')))\n",
    "model_8.add(TimeDistributed(BatchNormalization()))\n",
    "model_8.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "model_8.add(TimeDistributed(Conv2D(64, (3, 3) , padding='same', activation='relu')))\n",
    "model_8.add(TimeDistributed(BatchNormalization()))\n",
    "model_8.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "model_8.add(TimeDistributed(Conv2D(128, (3, 3) , padding='same', activation='relu')))\n",
    "model_8.add(TimeDistributed(BatchNormalization()))\n",
    "model_8.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "model_8.add(TimeDistributed(Conv2D(256, (3, 3) , padding='same', activation='relu')))\n",
    "model_8.add(TimeDistributed(BatchNormalization()))\n",
    "model_8.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "model_8.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model_8.add(LSTM(128))\n",
    "\n",
    "model_8.add(Dense(128,activation='relu'))\n",
    "model_8.add(Dropout(0.3))\n",
    "\n",
    "model_8.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "optimiser = tf.keras.optimizers.Adam()\n",
    "model_8.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model_8.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T08:42:07.815907Z",
     "iopub.status.busy": "2022-02-20T08:42:07.814010Z",
     "iopub.status.idle": "2022-02-20T09:55:24.841358Z",
     "shell.execute_reply": "2022-02-20T09:55:24.839614Z",
     "shell.execute_reply.started": "2022-02-20T08:42:07.815858Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /kaggle/input/gesture-recognition/Project_data/train ; batch size = 10\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.4838 - categorical_accuracy: 0.3597Source path =  /kaggle/input/gesture-recognition/Project_data/val ; batch size = 10\n",
      "67/67 [==============================] - 224s 3s/step - loss: 1.4838 - categorical_accuracy: 0.3597 - val_loss: 1.7960 - val_categorical_accuracy: 0.1800\n",
      "\n",
      "Epoch 00001: saving model to /kaggle/working/models/model_init_2022-02-2003_02_53.824697/model-00001-1.48384-0.35970-1.79605-0.18000.h5\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 233s 4s/step - loss: 1.2171 - categorical_accuracy: 0.4955 - val_loss: 1.8753 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00002: saving model to /kaggle/working/models/model_init_2022-02-2003_02_53.824697/model-00002-1.21712-0.49552-1.87529-0.25000.h5\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 217s 3s/step - loss: 0.9932 - categorical_accuracy: 0.6045 - val_loss: 2.1078 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00003: saving model to /kaggle/working/models/model_init_2022-02-2003_02_53.824697/model-00003-0.99317-0.60448-2.10781-0.23000.h5\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 218s 3s/step - loss: 0.9829 - categorical_accuracy: 0.6045 - val_loss: 1.7387 - val_categorical_accuracy: 0.3600\n",
      "\n",
      "Epoch 00004: saving model to /kaggle/working/models/model_init_2022-02-2003_02_53.824697/model-00004-0.98287-0.60448-1.73866-0.36000.h5\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 218s 3s/step - loss: 0.9108 - categorical_accuracy: 0.6269 - val_loss: 1.9048 - val_categorical_accuracy: 0.2900\n",
      "\n",
      "Epoch 00005: saving model to /kaggle/working/models/model_init_2022-02-2003_02_53.824697/model-00005-0.91077-0.62687-1.90482-0.29000.h5\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 218s 3s/step - loss: 1.1389 - categorical_accuracy: 0.5284 - val_loss: 1.5886 - val_categorical_accuracy: 0.2700\n",
      "\n",
      "Epoch 00006: saving model to /kaggle/working/models/model_init_2022-02-2003_02_53.824697/model-00006-1.13890-0.52836-1.58863-0.27000.h5\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 220s 3s/step - loss: 1.0880 - categorical_accuracy: 0.5567 - val_loss: 1.3824 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00007: saving model to /kaggle/working/models/model_init_2022-02-2003_02_53.824697/model-00007-1.08796-0.55672-1.38239-0.40000.h5\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 219s 3s/step - loss: 1.0105 - categorical_accuracy: 0.6015 - val_loss: 1.0800 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00008: saving model to /kaggle/working/models/model_init_2022-02-2003_02_53.824697/model-00008-1.01053-0.60149-1.07996-0.52000.h5\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 221s 3s/step - loss: 0.9314 - categorical_accuracy: 0.6403 - val_loss: 0.9248 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00009: saving model to /kaggle/working/models/model_init_2022-02-2003_02_53.824697/model-00009-0.93143-0.64030-0.92477-0.60000.h5\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 220s 3s/step - loss: 0.8225 - categorical_accuracy: 0.6776 - val_loss: 1.3642 - val_categorical_accuracy: 0.4600\n",
      "\n",
      "Epoch 00010: saving model to /kaggle/working/models/model_init_2022-02-2003_02_53.824697/model-00010-0.82252-0.67761-1.36423-0.46000.h5\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 217s 3s/step - loss: 0.8266 - categorical_accuracy: 0.6925 - val_loss: 1.0804 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00011: saving model to /kaggle/working/models/model_init_2022-02-2003_02_53.824697/model-00011-0.82662-0.69254-1.08044-0.55000.h5\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 220s 3s/step - loss: 0.7750 - categorical_accuracy: 0.6746 - val_loss: 2.2152 - val_categorical_accuracy: 0.2600\n",
      "\n",
      "Epoch 00012: saving model to /kaggle/working/models/model_init_2022-02-2003_02_53.824697/model-00012-0.77503-0.67463-2.21519-0.26000.h5\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 223s 3s/step - loss: 0.8364 - categorical_accuracy: 0.6761 - val_loss: 1.8798 - val_categorical_accuracy: 0.3400\n",
      "\n",
      "Epoch 00013: saving model to /kaggle/working/models/model_init_2022-02-2003_02_53.824697/model-00013-0.83637-0.67612-1.87975-0.34000.h5\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 217s 3s/step - loss: 0.6930 - categorical_accuracy: 0.7284 - val_loss: 0.9384 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00014: saving model to /kaggle/working/models/model_init_2022-02-2003_02_53.824697/model-00014-0.69303-0.72836-0.93838-0.62000.h5\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 217s 3s/step - loss: 0.6471 - categorical_accuracy: 0.7522 - val_loss: 0.9731 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00015: saving model to /kaggle/working/models/model_init_2022-02-2003_02_53.824697/model-00015-0.64707-0.75224-0.97312-0.61000.h5\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 216s 3s/step - loss: 0.5854 - categorical_accuracy: 0.7925 - val_loss: 0.8299 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00016: saving model to /kaggle/working/models/model_init_2022-02-2003_02_53.824697/model-00016-0.58535-0.79254-0.82991-0.62000.h5\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 219s 3s/step - loss: 0.5278 - categorical_accuracy: 0.8075 - val_loss: 0.9399 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00017: saving model to /kaggle/working/models/model_init_2022-02-2003_02_53.824697/model-00017-0.52780-0.80746-0.93986-0.65000.h5\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 218s 3s/step - loss: 0.4703 - categorical_accuracy: 0.8463 - val_loss: 0.8625 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00018: saving model to /kaggle/working/models/model_init_2022-02-2003_02_53.824697/model-00018-0.47034-0.84627-0.86247-0.64000.h5\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 218s 3s/step - loss: 0.4465 - categorical_accuracy: 0.8448 - val_loss: 0.9472 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00019: saving model to /kaggle/working/models/model_init_2022-02-2003_02_53.824697/model-00019-0.44652-0.84478-0.94716-0.58000.h5\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 217s 3s/step - loss: 0.4445 - categorical_accuracy: 0.8448 - val_loss: 0.9882 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00020: saving model to /kaggle/working/models/model_init_2022-02-2003_02_53.824697/model-00020-0.44448-0.84478-0.98820-0.67000.h5\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n"
     ]
    }
   ],
   "source": [
    "history_8 = train_model(model_8, batch_size = 10, num_epochs = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-9 (CNN + LSTM) with Transfer learning\n",
    "\n",
    "We have used the MobileNet pre-trained model as it has the 'Top-1' and 'Top-5' accuracy is almost similar to that of VGG-16 but with much lesser parameters just 4.3M as compared to VGG-16 which has 138.4M parameters. MobileNet also has a faster inference speed. (Source: https://keras.io/api/applications/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T17:11:20.306689Z",
     "iopub.status.busy": "2022-02-20T17:11:20.306148Z",
     "iopub.status.idle": "2022-02-20T17:11:20.810146Z",
     "shell.execute_reply": "2022-02-20T17:11:20.809393Z",
     "shell.execute_reply.started": "2022-02-20T17:11:20.306650Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf_no_top.h5\n",
      "17227776/17225924 [==============================] - 2s 0us/step\n",
      "17235968/17225924 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import mobilenet\n",
    "mobilenet_model = mobilenet.MobileNet(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T17:11:21.883593Z",
     "iopub.status.busy": "2022-02-20T17:11:21.882818Z",
     "iopub.status.idle": "2022-02-20T17:11:22.342977Z",
     "shell.execute_reply": "2022-02-20T17:11:22.342240Z",
     "shell.execute_reply.started": "2022-02-20T17:11:21.883537Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_33 (TimeDis (None, 30, 3, 3, 1024)    3228864   \n",
      "_________________________________________________________________\n",
      "time_distributed_34 (TimeDis (None, 30, 3, 3, 1024)    4096      \n",
      "_________________________________________________________________\n",
      "time_distributed_35 (TimeDis (None, 30, 1, 1, 1024)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_36 (TimeDis (None, 30, 1024)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               590336    \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 3,840,453\n",
      "Trainable params: 609,541\n",
      "Non-trainable params: 3,230,912\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_9 = Sequential()\n",
    "model_9.add(TimeDistributed(mobilenet_model,input_shape = (num_frames_to_sample, img_height, img_width, 3)))\n",
    "\n",
    "for layer in model_9.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model_9.add(TimeDistributed(BatchNormalization()))\n",
    "model_9.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model_9.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model_9.add(LSTM(128))\n",
    "model_9.add(Dropout(0.25))\n",
    "\n",
    "model_9.add(Dense(128,activation='relu'))\n",
    "model_9.add(Dropout(0.25))\n",
    "\n",
    "model_9.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "optimiser = tf.keras.optimizers.Adam()\n",
    "model_9.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model_9.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T17:11:31.828327Z",
     "iopub.status.busy": "2022-02-20T17:11:31.827798Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /kaggle/input/gesture-recognition/Project_data/train ; batch size = 10\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.4951 - categorical_accuracy: 0.3478Source path =  /kaggle/input/gesture-recognition/Project_data/val ; batch size = 10\n",
      "67/67 [==============================] - 220s 3s/step - loss: 1.4951 - categorical_accuracy: 0.3478 - val_loss: 1.2630 - val_categorical_accuracy: 0.4500\n",
      "\n",
      "Epoch 00001: saving model to /kaggle/working/models/model_init_2022-02-2010_48_02.580195/model-00001-1.49509-0.34776-1.26295-0.45000.h5\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 214s 3s/step - loss: 0.9218 - categorical_accuracy: 0.6791 - val_loss: 0.8342 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00002: saving model to /kaggle/working/models/model_init_2022-02-2010_48_02.580195/model-00002-0.92179-0.67910-0.83417-0.63000.h5\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 213s 3s/step - loss: 0.6202 - categorical_accuracy: 0.7851 - val_loss: 0.6480 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00003: saving model to /kaggle/working/models/model_init_2022-02-2010_48_02.580195/model-00003-0.62016-0.78507-0.64799-0.75000.h5\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 217s 3s/step - loss: 0.3922 - categorical_accuracy: 0.8672 - val_loss: 0.6500 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00004: saving model to /kaggle/working/models/model_init_2022-02-2010_48_02.580195/model-00004-0.39218-0.86716-0.64999-0.78000.h5\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 218s 3s/step - loss: 0.2231 - categorical_accuracy: 0.9343 - val_loss: 0.8766 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00005: saving model to /kaggle/working/models/model_init_2022-02-2010_48_02.580195/model-00005-0.22315-0.93433-0.87661-0.68000.h5\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 223s 3s/step - loss: 0.1632 - categorical_accuracy: 0.9507 - val_loss: 0.8799 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00006: saving model to /kaggle/working/models/model_init_2022-02-2010_48_02.580195/model-00006-0.16320-0.95075-0.87985-0.66000.h5\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 216s 3s/step - loss: 0.1333 - categorical_accuracy: 0.9612 - val_loss: 0.6845 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00007: saving model to /kaggle/working/models/model_init_2022-02-2010_48_02.580195/model-00007-0.13331-0.96119-0.68452-0.75000.h5\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 215s 3s/step - loss: 0.0814 - categorical_accuracy: 0.9866 - val_loss: 0.8135 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00008: saving model to /kaggle/working/models/model_init_2022-02-2010_48_02.580195/model-00008-0.08143-0.98657-0.81352-0.73000.h5\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 218s 3s/step - loss: 0.0877 - categorical_accuracy: 0.9851 - val_loss: 1.0358 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00009: saving model to /kaggle/working/models/model_init_2022-02-2010_48_02.580195/model-00009-0.08774-0.98507-1.03583-0.70000.h5\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 217s 3s/step - loss: 0.0705 - categorical_accuracy: 0.9896 - val_loss: 0.8402 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00010: saving model to /kaggle/working/models/model_init_2022-02-2010_48_02.580195/model-00010-0.07051-0.98955-0.84016-0.70000.h5\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 219s 3s/step - loss: 0.0553 - categorical_accuracy: 0.9925 - val_loss: 0.8137 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00011: saving model to /kaggle/working/models/model_init_2022-02-2010_48_02.580195/model-00011-0.05528-0.99254-0.81366-0.74000.h5\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 214s 3s/step - loss: 0.0570 - categorical_accuracy: 0.9985 - val_loss: 0.8495 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00012: saving model to /kaggle/working/models/model_init_2022-02-2010_48_02.580195/model-00012-0.05702-0.99851-0.84949-0.73000.h5\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 220s 3s/step - loss: 0.0348 - categorical_accuracy: 0.9955 - val_loss: 0.7864 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00013: saving model to /kaggle/working/models/model_init_2022-02-2010_48_02.580195/model-00013-0.03475-0.99552-0.78643-0.76000.h5\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 214s 3s/step - loss: 0.0291 - categorical_accuracy: 0.9985 - val_loss: 0.8166 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00014: saving model to /kaggle/working/models/model_init_2022-02-2010_48_02.580195/model-00014-0.02911-0.99851-0.81664-0.74000.h5\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 216s 3s/step - loss: 0.0642 - categorical_accuracy: 0.9910 - val_loss: 0.8767 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00015: saving model to /kaggle/working/models/model_init_2022-02-2010_48_02.580195/model-00015-0.06424-0.99104-0.87673-0.71000.h5\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 215s 3s/step - loss: 0.0576 - categorical_accuracy: 0.9955 - val_loss: 0.9014 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00016: saving model to /kaggle/working/models/model_init_2022-02-2010_48_02.580195/model-00016-0.05760-0.99552-0.90145-0.72000.h5\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 215s 3s/step - loss: 0.0578 - categorical_accuracy: 0.9940 - val_loss: 0.8280 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00017: saving model to /kaggle/working/models/model_init_2022-02-2010_48_02.580195/model-00017-0.05780-0.99403-0.82801-0.71000.h5\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 218s 3s/step - loss: 0.0502 - categorical_accuracy: 0.9940 - val_loss: 0.7674 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00018: saving model to /kaggle/working/models/model_init_2022-02-2010_48_02.580195/model-00018-0.05025-0.99403-0.76744-0.76000.h5\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 215s 3s/step - loss: 0.0423 - categorical_accuracy: 0.9970 - val_loss: 0.8021 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00019: saving model to /kaggle/working/models/model_init_2022-02-2010_48_02.580195/model-00019-0.04226-0.99701-0.80209-0.74000.h5\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 217s 3s/step - loss: 0.0484 - categorical_accuracy: 0.9970 - val_loss: 0.8292 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00020: saving model to /kaggle/working/models/model_init_2022-02-2010_48_02.580195/model-00020-0.04842-0.99701-0.82919-0.71000.h5\n"
     ]
    }
   ],
   "source": [
    "history_9= train_model(model_9, batch_size = 10, num_epochs = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-10 (CNN + LSTM) with Transfer learning and Training CNN layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T14:37:27.585178Z",
     "iopub.status.busy": "2022-02-20T14:37:27.584645Z",
     "iopub.status.idle": "2022-02-20T14:37:28.369970Z",
     "shell.execute_reply": "2022-02-20T14:37:28.369189Z",
     "shell.execute_reply.started": "2022-02-20T14:37:27.585140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import mobilenet\n",
    "mobilenet_model = mobilenet.MobileNet(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T14:37:37.201524Z",
     "iopub.status.busy": "2022-02-20T14:37:37.201279Z",
     "iopub.status.idle": "2022-02-20T14:37:37.655756Z",
     "shell.execute_reply": "2022-02-20T14:37:37.655052Z",
     "shell.execute_reply.started": "2022-02-20T14:37:37.201496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_41 (TimeDis (None, 30, 3, 3, 1024)    3228864   \n",
      "_________________________________________________________________\n",
      "time_distributed_42 (TimeDis (None, 30, 3, 3, 1024)    4096      \n",
      "_________________________________________________________________\n",
      "time_distributed_43 (TimeDis (None, 30, 1, 1, 1024)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_44 (TimeDis (None, 30, 1024)          0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 128)               590336    \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 3,840,453\n",
      "Trainable params: 3,816,517\n",
      "Non-trainable params: 23,936\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_10 = Sequential()\n",
    "model_10.add(TimeDistributed(mobilenet_model,input_shape = (num_frames_to_sample, img_height, img_width, 3)))\n",
    "\n",
    "for layer in model_10.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "model_10.add(TimeDistributed(BatchNormalization()))\n",
    "model_10.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model_10.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model_10.add(LSTM(128))\n",
    "model_10.add(Dropout(0.25))\n",
    "\n",
    "model_10.add(Dense(128,activation='relu'))\n",
    "model_10.add(Dropout(0.25))\n",
    "\n",
    "model_10.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "optimiser = tf.keras.optimizers.Adam()\n",
    "model_10.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model_10.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T14:37:40.080241Z",
     "iopub.status.busy": "2022-02-20T14:37:40.079969Z",
     "iopub.status.idle": "2022-02-20T15:51:55.469701Z",
     "shell.execute_reply": "2022-02-20T15:51:55.468937Z",
     "shell.execute_reply.started": "2022-02-20T14:37:40.080212Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /kaggle/input/gesture-recognition/Project_data/train ; batch size = 10\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.3385 - categorical_accuracy: 0.4343Source path =  /kaggle/input/gesture-recognition/Project_data/val ; batch size = 10\n",
      "67/67 [==============================] - 226s 3s/step - loss: 1.3385 - categorical_accuracy: 0.4343 - val_loss: 0.9031 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00001: saving model to /kaggle/working/models/model_init_2022-02-2014_37_40.081350/model-00001-1.33846-0.43433-0.90307-0.67000.h5\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 222s 3s/step - loss: 0.8510 - categorical_accuracy: 0.6791 - val_loss: 0.8744 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00002: saving model to /kaggle/working/models/model_init_2022-02-2014_37_40.081350/model-00002-0.85097-0.67910-0.87444-0.66000.h5\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 221s 3s/step - loss: 0.7106 - categorical_accuracy: 0.7716 - val_loss: 0.7269 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00003: saving model to /kaggle/working/models/model_init_2022-02-2014_37_40.081350/model-00003-0.71057-0.77164-0.72694-0.70000.h5\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 221s 3s/step - loss: 0.5065 - categorical_accuracy: 0.8209 - val_loss: 0.8004 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00004: saving model to /kaggle/working/models/model_init_2022-02-2014_37_40.081350/model-00004-0.50647-0.82090-0.80044-0.70000.h5\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 220s 3s/step - loss: 0.6907 - categorical_accuracy: 0.7582 - val_loss: 0.5284 - val_categorical_accuracy: 0.8100\n",
      "\n",
      "Epoch 00005: saving model to /kaggle/working/models/model_init_2022-02-2014_37_40.081350/model-00005-0.69068-0.75821-0.52843-0.81000.h5\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 221s 3s/step - loss: 0.6429 - categorical_accuracy: 0.7985 - val_loss: 0.8583 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00006: saving model to /kaggle/working/models/model_init_2022-02-2014_37_40.081350/model-00006-0.64288-0.79851-0.85832-0.72000.h5\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 221s 3s/step - loss: 0.6404 - categorical_accuracy: 0.7791 - val_loss: 0.3718 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00007: saving model to /kaggle/working/models/model_init_2022-02-2014_37_40.081350/model-00007-0.64041-0.77910-0.37175-0.82000.h5\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 222s 3s/step - loss: 0.4762 - categorical_accuracy: 0.8373 - val_loss: 0.5928 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00008: saving model to /kaggle/working/models/model_init_2022-02-2014_37_40.081350/model-00008-0.47620-0.83731-0.59282-0.79000.h5\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 220s 3s/step - loss: 0.3887 - categorical_accuracy: 0.8776 - val_loss: 0.3553 - val_categorical_accuracy: 0.9000\n",
      "\n",
      "Epoch 00009: saving model to /kaggle/working/models/model_init_2022-02-2014_37_40.081350/model-00009-0.38874-0.87761-0.35529-0.90000.h5\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 224s 3s/step - loss: 0.2998 - categorical_accuracy: 0.8970 - val_loss: 0.2810 - val_categorical_accuracy: 0.8700\n",
      "\n",
      "Epoch 00010: saving model to /kaggle/working/models/model_init_2022-02-2014_37_40.081350/model-00010-0.29984-0.89701-0.28102-0.87000.h5\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 223s 3s/step - loss: 0.3087 - categorical_accuracy: 0.9119 - val_loss: 0.3450 - val_categorical_accuracy: 0.8600\n",
      "\n",
      "Epoch 00011: saving model to /kaggle/working/models/model_init_2022-02-2014_37_40.081350/model-00011-0.30871-0.91194-0.34504-0.86000.h5\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 223s 3s/step - loss: 0.3406 - categorical_accuracy: 0.8955 - val_loss: 0.4907 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00012: saving model to /kaggle/working/models/model_init_2022-02-2014_37_40.081350/model-00012-0.34062-0.89552-0.49074-0.82000.h5\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 220s 3s/step - loss: 0.2794 - categorical_accuracy: 0.9060 - val_loss: 0.2792 - val_categorical_accuracy: 0.9000\n",
      "\n",
      "Epoch 00013: saving model to /kaggle/working/models/model_init_2022-02-2014_37_40.081350/model-00013-0.27944-0.90597-0.27922-0.90000.h5\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 221s 3s/step - loss: 0.3359 - categorical_accuracy: 0.9045 - val_loss: 0.5676 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00014: saving model to /kaggle/working/models/model_init_2022-02-2014_37_40.081350/model-00014-0.33589-0.90448-0.56755-0.80000.h5\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 222s 3s/step - loss: 0.4843 - categorical_accuracy: 0.8373 - val_loss: 1.1132 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00015: saving model to /kaggle/working/models/model_init_2022-02-2014_37_40.081350/model-00015-0.48425-0.83731-1.11322-0.72000.h5\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 221s 3s/step - loss: 0.3295 - categorical_accuracy: 0.8940 - val_loss: 0.3677 - val_categorical_accuracy: 0.8700\n",
      "\n",
      "Epoch 00016: saving model to /kaggle/working/models/model_init_2022-02-2014_37_40.081350/model-00016-0.32951-0.89403-0.36766-0.87000.h5\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 224s 3s/step - loss: 0.3678 - categorical_accuracy: 0.8910 - val_loss: 0.2692 - val_categorical_accuracy: 0.9000\n",
      "\n",
      "Epoch 00017: saving model to /kaggle/working/models/model_init_2022-02-2014_37_40.081350/model-00017-0.36782-0.89104-0.26923-0.90000.h5\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 226s 3s/step - loss: 0.2686 - categorical_accuracy: 0.9164 - val_loss: 0.1852 - val_categorical_accuracy: 0.9100\n",
      "\n",
      "Epoch 00018: saving model to /kaggle/working/models/model_init_2022-02-2014_37_40.081350/model-00018-0.26864-0.91642-0.18525-0.91000.h5\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 225s 3s/step - loss: 0.2269 - categorical_accuracy: 0.9254 - val_loss: 0.1431 - val_categorical_accuracy: 0.9400\n",
      "\n",
      "Epoch 00019: saving model to /kaggle/working/models/model_init_2022-02-2014_37_40.081350/model-00019-0.22685-0.92537-0.14308-0.94000.h5\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 225s 3s/step - loss: 0.3535 - categorical_accuracy: 0.8970 - val_loss: 0.2921 - val_categorical_accuracy: 0.8800\n",
      "\n",
      "Epoch 00020: saving model to /kaggle/working/models/model_init_2022-02-2014_37_40.081350/model-00020-0.35346-0.89701-0.29207-0.88000.h5\n"
     ]
    }
   ],
   "source": [
    "history_10 = train_model(model_10, batch_size = 10, num_epochs = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-11 (CNN + GRU) with Transfer learning and Training CNN layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T15:51:55.863169Z",
     "iopub.status.busy": "2022-02-20T15:51:55.862702Z",
     "iopub.status.idle": "2022-02-20T15:51:55.866811Z",
     "shell.execute_reply": "2022-02-20T15:51:55.866116Z",
     "shell.execute_reply.started": "2022-02-20T15:51:55.863132Z"
    }
   },
   "outputs": [],
   "source": [
    "num_frames_to_sample = 30\n",
    "img_height, img_width = 120, 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T15:51:55.869289Z",
     "iopub.status.busy": "2022-02-20T15:51:55.868814Z",
     "iopub.status.idle": "2022-02-20T15:52:26.662884Z",
     "shell.execute_reply": "2022-02-20T15:52:26.662064Z",
     "shell.execute_reply.started": "2022-02-20T15:51:55.869251Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import mobilenet\n",
    "mobilenet_model = mobilenet.MobileNet(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T15:52:26.664990Z",
     "iopub.status.busy": "2022-02-20T15:52:26.664730Z",
     "iopub.status.idle": "2022-02-20T15:52:27.161061Z",
     "shell.execute_reply": "2022-02-20T15:52:27.160239Z",
     "shell.execute_reply.started": "2022-02-20T15:52:26.664955Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_57 (TimeDis (None, 30, 3, 3, 1024)    3228864   \n",
      "_________________________________________________________________\n",
      "time_distributed_58 (TimeDis (None, 30, 3, 3, 1024)    4096      \n",
      "_________________________________________________________________\n",
      "time_distributed_59 (TimeDis (None, 30, 1, 1, 1024)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_60 (TimeDis (None, 30, 1024)          0         \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 128)               442752    \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 3,692,869\n",
      "Trainable params: 3,668,933\n",
      "Non-trainable params: 23,936\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_11 = Sequential()\n",
    "model_11.add(TimeDistributed(mobilenet_model, input_shape = (num_frames_to_sample, img_height, img_width, 3)))\n",
    "\n",
    "for layer in model_11.layers:\n",
    "    layer.trainable = True\n",
    "    \n",
    "model_11.add(TimeDistributed(BatchNormalization()))\n",
    "model_11.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model_11.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model_11.add(GRU(128))\n",
    "model_11.add(Dropout(0.25))\n",
    "\n",
    "model_11.add(Dense(128,activation='relu'))\n",
    "model_11.add(Dropout(0.25))\n",
    "\n",
    "model_11.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "optimiser = tf.keras.optimizers.Adam()\n",
    "model_11.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model_11.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T15:52:27.162750Z",
     "iopub.status.busy": "2022-02-20T15:52:27.162463Z",
     "iopub.status.idle": "2022-02-20T17:06:32.430938Z",
     "shell.execute_reply": "2022-02-20T17:06:32.430165Z",
     "shell.execute_reply.started": "2022-02-20T15:52:27.162712Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /kaggle/input/gesture-recognition/Project_data/train ; batch size = 10\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.2113 - categorical_accuracy: 0.5045Source path =  /kaggle/input/gesture-recognition/Project_data/val ; batch size = 10\n",
      "67/67 [==============================] - 227s 3s/step - loss: 1.2113 - categorical_accuracy: 0.5045 - val_loss: 0.7186 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00001: saving model to /kaggle/working/models/model_init_2022-02-2015_52_27.164407/model-00001-1.21134-0.50448-0.71856-0.74000.h5\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 220s 3s/step - loss: 0.5852 - categorical_accuracy: 0.7836 - val_loss: 0.6624 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00002: saving model to /kaggle/working/models/model_init_2022-02-2015_52_27.164407/model-00002-0.58517-0.78358-0.66243-0.78000.h5\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 218s 3s/step - loss: 0.3766 - categorical_accuracy: 0.8776 - val_loss: 0.3286 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00003: saving model to /kaggle/working/models/model_init_2022-02-2015_52_27.164407/model-00003-0.37662-0.87761-0.32865-0.83000.h5\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 223s 3s/step - loss: 0.3509 - categorical_accuracy: 0.8806 - val_loss: 0.5587 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00004: saving model to /kaggle/working/models/model_init_2022-02-2015_52_27.164407/model-00004-0.35094-0.88060-0.55869-0.79000.h5\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 221s 3s/step - loss: 0.2949 - categorical_accuracy: 0.9149 - val_loss: 0.5152 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00005: saving model to /kaggle/working/models/model_init_2022-02-2015_52_27.164407/model-00005-0.29486-0.91493-0.51518-0.78000.h5\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 223s 3s/step - loss: 0.2702 - categorical_accuracy: 0.9149 - val_loss: 0.5872 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00006: saving model to /kaggle/working/models/model_init_2022-02-2015_52_27.164407/model-00006-0.27025-0.91493-0.58716-0.79000.h5\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 223s 3s/step - loss: 0.2709 - categorical_accuracy: 0.9209 - val_loss: 0.2441 - val_categorical_accuracy: 0.9100\n",
      "\n",
      "Epoch 00007: saving model to /kaggle/working/models/model_init_2022-02-2015_52_27.164407/model-00007-0.27087-0.92090-0.24414-0.91000.h5\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 223s 3s/step - loss: 0.2051 - categorical_accuracy: 0.9433 - val_loss: 0.1032 - val_categorical_accuracy: 0.9400\n",
      "\n",
      "Epoch 00008: saving model to /kaggle/working/models/model_init_2022-02-2015_52_27.164407/model-00008-0.20514-0.94328-0.10320-0.94000.h5\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 222s 3s/step - loss: 0.2113 - categorical_accuracy: 0.9493 - val_loss: 0.5433 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00009: saving model to /kaggle/working/models/model_init_2022-02-2015_52_27.164407/model-00009-0.21132-0.94925-0.54329-0.82000.h5\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 222s 3s/step - loss: 0.1817 - categorical_accuracy: 0.9403 - val_loss: 0.4469 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00010: saving model to /kaggle/working/models/model_init_2022-02-2015_52_27.164407/model-00010-0.18174-0.94030-0.44690-0.83000.h5\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 222s 3s/step - loss: 0.1595 - categorical_accuracy: 0.9478 - val_loss: 0.3062 - val_categorical_accuracy: 0.9000\n",
      "\n",
      "Epoch 00011: saving model to /kaggle/working/models/model_init_2022-02-2015_52_27.164407/model-00011-0.15949-0.94776-0.30622-0.90000.h5\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 225s 3s/step - loss: 0.1768 - categorical_accuracy: 0.9493 - val_loss: 0.6747 - val_categorical_accuracy: 0.8100\n",
      "\n",
      "Epoch 00012: saving model to /kaggle/working/models/model_init_2022-02-2015_52_27.164407/model-00012-0.17685-0.94925-0.67468-0.81000.h5\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 225s 3s/step - loss: 0.0932 - categorical_accuracy: 0.9761 - val_loss: 0.0956 - val_categorical_accuracy: 0.9600\n",
      "\n",
      "Epoch 00013: saving model to /kaggle/working/models/model_init_2022-02-2015_52_27.164407/model-00013-0.09324-0.97612-0.09558-0.96000.h5\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 221s 3s/step - loss: 0.0798 - categorical_accuracy: 0.9896 - val_loss: 0.1021 - val_categorical_accuracy: 0.9600\n",
      "\n",
      "Epoch 00014: saving model to /kaggle/working/models/model_init_2022-02-2015_52_27.164407/model-00014-0.07975-0.98955-0.10206-0.96000.h5\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 220s 3s/step - loss: 0.0615 - categorical_accuracy: 0.9896 - val_loss: 0.1318 - val_categorical_accuracy: 0.9300\n",
      "\n",
      "Epoch 00015: saving model to /kaggle/working/models/model_init_2022-02-2015_52_27.164407/model-00015-0.06146-0.98955-0.13179-0.93000.h5\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 220s 3s/step - loss: 0.0477 - categorical_accuracy: 1.0000 - val_loss: 0.1330 - val_categorical_accuracy: 0.9400\n",
      "\n",
      "Epoch 00017: saving model to /kaggle/working/models/model_init_2022-02-2015_52_27.164407/model-00017-0.04771-1.00000-0.13305-0.94000.h5\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 221s 3s/step - loss: 0.0342 - categorical_accuracy: 0.9940 - val_loss: 0.0807 - val_categorical_accuracy: 0.9700\n",
      "\n",
      "Epoch 00018: saving model to /kaggle/working/models/model_init_2022-02-2015_52_27.164407/model-00018-0.03417-0.99403-0.08075-0.97000.h5\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 220s 3s/step - loss: 0.0546 - categorical_accuracy: 0.9940 - val_loss: 0.0683 - val_categorical_accuracy: 0.9700\n",
      "\n",
      "Epoch 00019: saving model to /kaggle/working/models/model_init_2022-02-2015_52_27.164407/model-00019-0.05457-0.99403-0.06833-0.97000.h5\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 220s 3s/step - loss: 0.0357 - categorical_accuracy: 0.9955 - val_loss: 0.0869 - val_categorical_accuracy: 0.9600\n",
      "\n",
      "Epoch 00020: saving model to /kaggle/working/models/model_init_2022-02-2015_52_27.164407/model-00020-0.03570-0.99552-0.08685-0.96000.h5\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history_11= train_model(model_11, batch_size = 10, num_epochs = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model performance that we achieved was using Pre-trained MobileNet CNN model and GRU network. In which we also trained the weights of the CNN model. (Model 11)\n",
    "\n",
    "`For Model 11:`\n",
    "   - Training Accuracy was 100% and Validation Accuracy was 99%\n",
    "   \n",
    "The weights for the same model can be found at \"model_5/model_init_2022-02-2015_52_27.164407/model-00016-0.06678-0.99403-0.04007-0.99000.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
